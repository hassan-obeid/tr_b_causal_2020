{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Purpose\n",
    "\n",
    "The purpose of this notebook is to demonstrate prior and predictive checks of one's causal graphical model.\n",
    "\n",
    "The prior checks are to be used as part of one's falsification efforts before estimating the posterior distribution of one's unknown model parameters. If one's causal model contains latent variables, then such prior checks are expected to be extremely valuable. They are expected to indicate when one's model is likely to poorly fit one's data. This information can be used to avoid a potentially lengthy model estimation process. These checks will likely be implemented with very liberal thresholds for deciding that a model is not even worth beign estimated.\n",
    "\n",
    "The posterior predictive checks are to really ensure that the observed data is well fit by the assumptions of one's causal model.\n",
    "\n",
    "# Logical steps\n",
    "0. Determine the test statistic to be computed.\n",
    "1. Require as inputs:\n",
    "   1. predictive samples of all model variables (latent and observed),\n",
    "   2. function to compute the desired test statistic given a sample from the causal graph,\n",
    "   3. the observed data.\n",
    "   4. function to plot the distribution of the simulated test statistic and the value/distribution of the observed test statistic.\n",
    "2. For each predictive sample,\n",
    "   1. Compute the value of the simulated and observed test statistic (assuming the observed test statistic also depends on the simulated values. If not, simply store the value of the observed test statistic and do not recompute it.)\n",
    "   2. Store the simulated and observed test statistics.\n",
    "3. Visualize the distribution of the simulated and observed test statistics.\n",
    "4. Produce a scalar summary of the distribution of simulated test statistics if desired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declare notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare hyperparameters for testing\n",
    "MIN_SAMPLES_LEAF = 40\n",
    "NUM_PERMUTATIONS = 100\n",
    "\n",
    "# Declare the columns to be used for testing\n",
    "x1_col = 'num_licensed_drivers'\n",
    "x2_col = 'num_cars'\n",
    "mode_id_col = 'mode_id'\n",
    "\n",
    "# Set the colors for plotting\n",
    "ORIG_COLOR = '#045a8d'\n",
    "SIMULATED_COLOR = '#a6bddb'\n",
    "\n",
    "# Declare paths to data\n",
    "DATA_PATH =\\\n",
    "    '../../data/raw/spring_2016_all_bay_area_long_format_plus_cross_bay_col.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute needed imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pdb import set_trace as bp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "import seaborn as sbn\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from causalgraphicalmodels import CausalGraphicalModel\n",
    "\n",
    "sys.path.insert(0, '../../src/')\n",
    "import viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create needed functions for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_regressor(x_2d, y, seed=None):\n",
    "    # regressor_kwargs =\\\n",
    "    #     regressor = LinearRegression()\n",
    "    #     {'min_samples_leaf': MIN_SAMPLES_LEAF,\n",
    "    #      'max_samples': 0.8}\n",
    "    # if seed is not None:\n",
    "    #     regressor_kwargs['random_state'] = seed + 10\n",
    "    # regressor =\\\n",
    "    #     RandomForestRegressor(**regressor_kwargs)\n",
    "    regressor = LinearRegression()\n",
    "    regressor.fit(x_2d, y)\n",
    "    return regressor\n",
    "\n",
    "\n",
    "def computed_vs_obs_r2(x1_array,\n",
    "                       x2_array,\n",
    "                       z_array,\n",
    "                       seed,\n",
    "                       progress=True):\n",
    "    # Combine the various predictors\n",
    "    combined_obs_predictors =\\\n",
    "        np.concatenate((x2_array[:, None], z_array[:, None]), axis=1)\n",
    "\n",
    "    # Determine the number of rows being plotted\n",
    "    num_rows = x1_array.shape[0]\n",
    "\n",
    "    # Create a regressor to be used to compute the conditional expectations\n",
    "    regressor = _make_regressor(combined_obs_predictors, x1_array, seed)\n",
    "\n",
    "    # Get the observed expectations\n",
    "    obs_expectation = regressor.predict(combined_obs_predictors)\n",
    "    obs_r2 = r2_score(x1_array, obs_expectation)\n",
    "\n",
    "    # Initialize arrays to store the permuted expectations and r2's\n",
    "    permuted_expectations = np.empty((num_rows, NUM_PERMUTATIONS))\n",
    "    permuted_r2 = np.empty(NUM_PERMUTATIONS, dtype=float)\n",
    "\n",
    "    # Get the permuted expectations\n",
    "    shuffled_index_array = np.arange(num_rows)\n",
    "\n",
    "    iterable = range(NUM_PERMUTATIONS)\n",
    "    if progress:\n",
    "        iterable = tqdm(iterable)\n",
    "\n",
    "    for i in iterable:\n",
    "        # Shuffle the index array\n",
    "        np.random.shuffle(shuffled_index_array)\n",
    "        # Get the new set of permuted X_2 values\n",
    "        current_x2 = x2_array[shuffled_index_array]\n",
    "        # Get the current combined predictors\n",
    "        current_predictors =\\\n",
    "            np.concatenate((current_x2[:, None], obs_z[:, None]), axis=1)\n",
    "        # Fit a new model and store the current expectation\n",
    "        current_regressor =\\\n",
    "            _make_regressor(current_predictors, x1_array, seed)\n",
    "        permuted_expectations[:, i] =\\\n",
    "            current_regressor.predict(current_predictors)\n",
    "        permuted_r2[i] = r2_score(x1_array, permuted_expectations[:, i])\n",
    "    return obs_r2, permuted_r2\n",
    "\n",
    "\n",
    "def compute_pvalue(obs_r2, permuted_r2):\n",
    "    return (obs_r2 < permuted_r2).mean()\n",
    "\n",
    "\n",
    "def compute_predictive_independence_test_values(samples, obs_sample, seed):\n",
    "    \"\"\"\n",
    "    test_values = p-values of conditional independence test\n",
    "    \"\"\"\n",
    "    # Determine the number of samples in order to create an iterable for\n",
    "    # getting and storing test samples\n",
    "    if len(samples.shape != 3):\n",
    "        msg = '`samples` should have shape (num_rows, 3, num_samples).'\n",
    "        raise ValueError(msg)\n",
    "    num_samples = samples.shape[-1]\n",
    "\n",
    "    # Initialize a container for the p-values of the sampled and observed data\n",
    "    sampled_pvals = np.empty((num_samples,), dtype=float)\n",
    "    obs_pvals = np.empty((num_samples,), dtype=float)\n",
    "\n",
    "    # Create the iterable to be looped over to compute test values\n",
    "    iterable = range(NUM_PERMUTATIONS)\n",
    "    if progress:\n",
    "        iterable = tqdm(iterable)\n",
    "\n",
    "    # Populate the arrays of test statistics\n",
    "    for i in iterable:\n",
    "        # Get the data to be used to calculate this set of p-values\n",
    "        current_sim_sample = samples[:, :, i]\n",
    "        current_sim_z = current_sim_sample[:, -1]\n",
    "        current_augmented_obs = np.concatenate((obs_samples, current_sim_z))\n",
    "\n",
    "        # Package the arguments to compute the predictive r2 values\n",
    "        sim_args =\\\n",
    "            (current_sim_sample[:, 0],\n",
    "             current_sim_sample[:, 1],\n",
    "             current_sim_z,\n",
    "             seed,\n",
    "             False\n",
    "            )\n",
    "\n",
    "        augmented_obs_args =\\\n",
    "            (current_augmented_obs[:, 0],\n",
    "             current_augmented_obs[:, 1],\n",
    "             current_augmented_obs[:, 2],\n",
    "             seed,\n",
    "             False\n",
    "            )\n",
    "\n",
    "        # Compute and store the p-values of the conditional independence\n",
    "        # test for the current simulated and augmented dataset\n",
    "        sampled_pvals[i] =\\\n",
    "            compute_pvalue(computed_vs_obs_r2(*sim_args))\n",
    "\n",
    "        obs_pvals[i] =\\\n",
    "            compute_pvalue(computed_vs_obs_r2(*augmented_obs_args))\n",
    "    return sampled_pvals, obs_pvals\n",
    "\n",
    "\n",
    "def visualize_predictive_cit_results(\n",
    "        sampled_pvals,\n",
    "        obs_pvals,\n",
    "        verbose=True,\n",
    "        show=True,\n",
    "        close=False):\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    overall_p_value = (obs_pvals < sampled_pvals).mean()\n",
    "\n",
    "    if verbose:\n",
    "        msg =\\\n",
    "            'The p-value of the predictive, permutation C.I.T. is {:.2f}.'\n",
    "        print(msg.format(overall_p_value))\n",
    "\n",
    "    sbn.kdeplot(\n",
    "        sampled_pvals, ax=ax, color=SIMULATED_COLOR, label='Simulated')\n",
    "    sbn.kdeplot(\n",
    "        obs_pvals, ax=ax, color=ORIG_COLOR, label='Observed')\n",
    "\n",
    "    ax.set_xlabel('Permutation P-value', fontsize=13)\n",
    "    ax.set_ylabel(\n",
    "        'Density', fontdict={'fontsize':13, 'rotation':0}, labelpad=40)\n",
    "    ax.legend(loc='best')\n",
    "    sbn.despine()\n",
    "    if show:\n",
    "        fig.show()\n",
    "    if close:\n",
    "        plt.close(fig=fig)\n",
    "    return overall_p_value\n",
    "\n",
    "\n",
    "def perform_visual_predictive_cit_test(\n",
    "        samples,\n",
    "        obs_sample,\n",
    "        seed=1038,\n",
    "        verbose=True,\n",
    "        show=True,\n",
    "        close=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    samples : 3D ndarray of shape (num_rows, 3, num_samples).\n",
    "        Columns should contain, in order, simulated x1, x2, z.\n",
    "    obs_sample : 2D ndarray of shape (num_rows, 2)\n",
    "        Columns should contain, in order, observed x1, observed x2.\n",
    "    \"\"\"\n",
    "    # Set a random seed for reproducibility\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Compute the observed and sampled pvalues\n",
    "    sampled_pvals, obs_pvals =\\\n",
    "        compute_predictive_independence_test_values(\n",
    "            samples, obs_sample, seed)\n",
    "\n",
    "    # Visualize the results of the predictive permutation CIT test\n",
    "    overall_p_value =\\\n",
    "        visualize_predictive_cit_results(\n",
    "            sampled_pvals, obs_pvals, verbose=verbose, show=show, close=close)\n",
    "    return p_value\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract data for the factor model checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data\n",
    "df = pd.read_csv(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the values to be used for testing\n",
    "filter_array = (df[mode_id_col] == 1).values\n",
    "obs_x1 = df[x1_col].values[filter_array]\n",
    "obs_x2 = df[x2_col].values[filter_array]\n",
    "obs_sample =\\\n",
    "    np.concatenate((obs_x1[:, None], obs_x2[:, None]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"718pt\" height=\"188pt\"\n",
       " viewBox=\"0.00 0.00 717.93 188.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 184)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-184 713.9326,-184 713.9326,4 -4,4\"/>\n",
       "<!-- Total Travel Distance -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>Total Travel Distance</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"152.4065\" cy=\"-162\" rx=\"86.2541\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"152.4065\" y=\"-157.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Total Travel Distance</text>\n",
       "</g>\n",
       "<!-- Total Travel Cost -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>Total Travel Cost</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"71.4065\" cy=\"-90\" rx=\"71.3132\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"71.4065\" y=\"-85.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Total Travel Cost</text>\n",
       "</g>\n",
       "<!-- Total Travel Distance&#45;&gt;Total Travel Cost -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>Total Travel Distance&#45;&gt;Total Travel Cost</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M132.3841,-144.2022C122.2055,-135.1546 109.6852,-124.0255 98.6434,-114.2105\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"100.7186,-111.3723 90.9192,-107.3446 96.068,-116.6042 100.7186,-111.3723\"/>\n",
       "</g>\n",
       "<!-- Total Travel Time -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>Total Travel Time</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"234.4065\" cy=\"-90\" rx=\"73.2773\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"234.4065\" y=\"-85.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Total Travel Time</text>\n",
       "</g>\n",
       "<!-- Total Travel Distance&#45;&gt;Total Travel Time -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>Total Travel Distance&#45;&gt;Total Travel Time</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M172.6762,-144.2022C182.9804,-135.1546 195.6553,-124.0255 206.8334,-114.2105\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"209.4479,-116.5727 214.653,-107.3446 204.8293,-111.3126 209.4479,-116.5727\"/>\n",
       "</g>\n",
       "<!-- Utility (Drive Alone) -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>Utility (Drive Alone)</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"315.4065\" cy=\"-18\" rx=\"84.5514\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"315.4065\" y=\"-13.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Utility (Drive Alone)</text>\n",
       "</g>\n",
       "<!-- Total Travel Cost&#45;&gt;Utility (Drive Alone) -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>Total Travel Cost&#45;&gt;Utility (Drive Alone)</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M117.8754,-76.2879C157.1285,-64.705 213.5801,-48.0471 256.0906,-35.5031\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"257.1555,-38.8381 265.7561,-32.651 255.1744,-32.1243 257.1555,-38.8381\"/>\n",
       "</g>\n",
       "<!-- Number of Autos -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>Number of Autos</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"397.4065\" cy=\"-90\" rx=\"71.5376\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"397.4065\" y=\"-85.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Number of Autos</text>\n",
       "</g>\n",
       "<!-- Number of Autos&#45;&gt;Utility (Drive Alone) -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>Number of Autos&#45;&gt;Utility (Drive Alone)</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M377.5566,-72.5708C367.1585,-63.4408 354.2612,-52.1163 342.9152,-42.1539\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"345.1816,-39.4862 335.3579,-35.5182 340.563,-44.7463 345.1816,-39.4862\"/>\n",
       "</g>\n",
       "<!-- Number of Licensed Drivers -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>Number of Licensed Drivers</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"598.4065\" cy=\"-90\" rx=\"111.552\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"598.4065\" y=\"-85.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Number of Licensed Drivers</text>\n",
       "</g>\n",
       "<!-- Number of Licensed Drivers&#45;&gt;Utility (Drive Alone) -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>Number of Licensed Drivers&#45;&gt;Utility (Drive Alone)</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M538.3895,-74.7307C491.8865,-62.8995 427.572,-46.5368 379.96,-34.4235\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"380.6204,-30.9801 370.0661,-31.9063 378.8944,-37.764 380.6204,-30.9801\"/>\n",
       "</g>\n",
       "<!-- Total Travel Time&#45;&gt;Utility (Drive Alone) -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>Total Travel Time&#45;&gt;Utility (Drive Alone)</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M254.0144,-72.5708C264.1869,-63.5286 276.781,-52.3338 287.9097,-42.4417\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"290.5497,-44.7778 295.6985,-35.5182 285.8991,-39.546 290.5497,-44.7778\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x1a1b9f6bd0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note the variables that take part in the drive alone utility\n",
    "# The following cell is taken from 5.0pmab-simulation-causal-graph.ipynb\n",
    "V_Drive_Alone =\\\n",
    "    CausalGraphicalModel(\n",
    "        nodes=[\"Total Travel Distance\",\n",
    "               \"Total Travel Time\",\n",
    "               \"Total Travel Cost\",\n",
    "               \"Number of Autos\",\n",
    "               \"Number of Licensed Drivers\",\n",
    "               \"Utility (Drive Alone)\"],\n",
    "         edges=[(\"Total Travel Distance\",\"Total Travel Time\"),\n",
    "                (\"Total Travel Distance\",\"Total Travel Cost\"),\n",
    "                (\"Total Travel Time\", \"Utility (Drive Alone)\"), \n",
    "                (\"Total Travel Cost\", \"Utility (Drive Alone)\"), \n",
    "                (\"Number of Autos\", \"Utility (Drive Alone)\"),\n",
    "                (\"Number of Licensed Drivers\",\"Utility (Drive Alone)\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# draw the causal model\n",
    "V_Drive_Alone.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      total_travel_distance  total_travel_cost  total_travel_time  num_cars  \\\n",
      "mean              15.579498           3.289287          29.667137  2.050999   \n",
      "std               18.465879           4.214448          24.586569  0.880458   \n",
      "\n",
      "      num_licensed_drivers  \n",
      "mean              2.178759  \n",
      "std               0.751621  \n"
     ]
    }
   ],
   "source": [
    "# Create a list of the variables in the drive alone utility\n",
    "drive_alone_variables =\\\n",
    "    ['total_travel_distance',\n",
    "     'total_travel_cost',\n",
    "     'total_travel_time',\n",
    "     'num_cars',\n",
    "     'num_licensed_drivers'\n",
    "    ]\n",
    "\n",
    "# Create a sub-dataframe with those variables\n",
    "drive_alone_df =\\\n",
    "    df.loc[df['mode_id'] == 1, drive_alone_variables]\n",
    "\n",
    "# Get the means and standard deviations of those variables\n",
    "drive_alone_means = drive_alone_df.mean()\n",
    "drive_alone_means.name = 'mean'\n",
    "\n",
    "drive_alone_stds = drive_alone_df.std()\n",
    "drive_alone_stds.name = 'std'\n",
    "\n",
    "# Look at the computed means and standard deviations\n",
    "print(pd.DataFrame([drive_alone_means, drive_alone_stds]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the factor model that is to be checked\n",
    "\n",
    "In Wang and Blei's deconfounder technique, we fit a factor model to the variables in one's outcome model.\n",
    "\n",
    "The factor model being considered here is:\n",
    "\n",
    "$\n",
    "\\begin{aligned}\n",
    "X_{\\textrm{standardized}} &= Z * W + \\epsilon\\\\\n",
    "\\textrm{where } \\epsilon &= \\left[ \\epsilon_1, \\epsilon_2, ..., \\epsilon_D \\right]\\\\\n",
    "\\epsilon_d &\\in \\mathbb{R}^{\\textrm{N x 1}}\\\\\n",
    "\\epsilon_d &\\sim \\mathcal{N} \\left(0, \\sigma \\right) \\forall d \\in \\left\\lbrace 1, 2, ... D \\right\\rbrace\\\\\n",
    "Z &\\in \\mathbb{R}^{\\textrm{N x 1}}\\\\\n",
    "Z &\\sim \\mathcal{N} \\left(0, 1 \\right)\\\\\n",
    "W &\\in \\mathbb{R}^{1 x D}\\\\\n",
    "W &\\sim \\mathcal{N} \\left(0, 1 \\right)\\\\\n",
    "N &= \\textrm{Number of rows in X_standardized}\\\\\n",
    "D &= \\textrm{Number of columns in X_standardized}\n",
    "\\end{aligned}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the prior for the factor model of the standardized drive alone dataframe\n",
    "w_prior_dist = norm()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md,py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
