{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "import random\n",
    "from fitter import Fitter\n",
    "import attr\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution Fitting Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FitDistribution(object):\n",
    "    \"\"\"Fit and simulate data to known distributions.\n",
    "    \n",
    "    Input:\n",
    "    ------\n",
    "    - data: array-like or dataframe.\n",
    "    - dists: list.\n",
    "        This parameter contains a list of distributions to be explored.\n",
    "        When None, every available distribution on scipy is explored.\n",
    "    - bins: int. \n",
    "        Numbers of bins to be used for the cumulative histogram. This has\n",
    "        an impact on the quality of the fit.\n",
    "    - timeout: int. \n",
    "        Maximum time for a given distribution. If timeout is reached, \n",
    "        the distribution is skipped.\n",
    "        \"\"\"\n",
    "    def __init__(self, data, dists=None, timeout=30, verbose=False, bins=100):\n",
    "        self.data = data\n",
    "        #self.var_types = var_types\n",
    "        self.dists = dists\n",
    "        self.timeout = timeout\n",
    "        self.verbose = verbose\n",
    "        self.bins = bins\n",
    "        self.ArrayDistDict = defaultdict()\n",
    "        self.params_dict = defaultdict(dict)\n",
    "\n",
    "    def FindArrayDist(self, cat_var):\n",
    "        \"\"\"Function to extract the best distribution for a specified array.\n",
    "        Uses the fit method from the Fitter module in the fitter library\n",
    "        Inputs:\n",
    "        -------\n",
    "        - cat_var: boolean\n",
    "            Boolean to signify whether the variable to be simulated\n",
    "            is discrete/categorical or continuous.\n",
    "        \n",
    "        Outputs:\n",
    "        -------\n",
    "        By default, the function returns a dictionary with best distribution name\n",
    "        and parameters associated with it. If a number of distributions\n",
    "        was specified, the function returns a pandas DataFrame with\n",
    "        the N best distributions, along with a plot showing all of them.\"\"\"\n",
    "        self.ArrayDistDict = dict()\n",
    "        if  cat_var == True:\n",
    "            self.ArrayDistDict['distribution'] = 'categorical'\n",
    "            np_array_range = np.arange(self.data.max()+1)\n",
    "            array_bincount = np.bincount(self.data)\n",
    "            probs = array_bincount / len(self.data)\n",
    "            \n",
    "            self.ArrayDistDict['parameters'] = [np_array_range,\n",
    "                                                probs]            \n",
    "        else:\n",
    "            fitter_object = Fitter(data=self.data,\n",
    "                                   distributions=self.dists,\n",
    "                                   timeout=self.timeout)\n",
    "            fitter_object.fit()\n",
    "            BestDict = fitter_object.get_best()\n",
    "            self.ArrayDistDict['distribution'] = list(BestDict.items())[0][0]\n",
    "            self.ArrayDistDict['parameters'] = list(BestDict.items())[0][1]\n",
    "        return self.ArrayDistDict\n",
    "    \n",
    "    def SimArray(self, size=100):\n",
    "        \"\"\"Function to simulate data for an array based on the best fitted\n",
    "        distribution.\n",
    "        Input:\n",
    "        -----\n",
    "        - size : int\n",
    "                size of the array to be simulated.\n",
    "        Outputs:\n",
    "        -------\n",
    "        Simulated array based on the best fit distribution.\"\"\"\n",
    "        if self.ArrayDistDict['distribution'] == 'categorical':\n",
    "            Sim_Array = np.random.choice(a=self.ArrayDistDict['parameters'][0],\n",
    "                                         p=self.ArrayDistDict['parameters'][1],\n",
    "                                         size=size)\n",
    "        else:\n",
    "            dist = getattr(scipy.stats, self.ArrayDistDict['distribution'])\n",
    "            Sim_Array = dist.rvs(*self.ArrayDistDict['parameters'], size=size)\n",
    "        return Sim_Array\n",
    "    \n",
    "    def FindDfDist(self, var_types):\n",
    "        \"\"\"Function to extract the best distribution from a specified dataframe.\n",
    "        Uses the function find_dist, which in turn uses the fit method from the\n",
    "        Fitter module in the fitter library\n",
    "        Inputs:\n",
    "        -------\n",
    "        - var_types: dictionary\n",
    "            Dictionary with keys as column names for dataset variables, the value\n",
    "            of each key is a string showing whether the variable is discrete/cat\n",
    "            or continuous.\n",
    "\n",
    "        Outputs:\n",
    "        -------\n",
    "        *FOR NOW*, the function returns a dictionary showing the best distribution\n",
    "        name for each array in the dataframe and parameters associated with it.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        for column in list(self.data.columns):\n",
    "            if  var_types[column] == 'categorical':\n",
    "                self.params_dict[column]['distribution'] = 'categorical'\n",
    "                np_array_range = np.arange(self.data[column].max()+1)\n",
    "                array_bincount = np.bincount(self.data[column])\n",
    "                probs = array_bincount / len(self.data[column])\n",
    "                self.params_dict[column]['parameters'] = [np_array_range,\n",
    "                                                          probs]            \n",
    "            else:\n",
    "                fitter_object = Fitter(data=self.data[column],\n",
    "                                       distributions=self.dists,\n",
    "                                       timeout=self.timeout)\n",
    "                fitter_object.fit()\n",
    "                BestDict = fitter_object.get_best()\n",
    "                self.params_dict[column]['distribution'] = list(BestDict.items())[0][0]\n",
    "                self.params_dict[column]['parameters'] = list(BestDict.items())[0][1]\n",
    "        return self.params_dict\n",
    "\n",
    "    def SimDf(self, size=1000):\n",
    "        \"\"\"Funtion to simulate data of size N based on specified\n",
    "        distribution/parameters found by the fitter package.\n",
    "        Inputs:\n",
    "        -------\n",
    "        data: dataframe from which columns are to be taken\n",
    "        dist_params: the distribution parameters from find_dist_df\n",
    "        Outputs:\n",
    "        -------\n",
    "        DataFrame object with simulated data based on specified distributions\n",
    "        \"\"\"\n",
    "        Sim_Df = pd.DataFrame(columns=list(self.params_dict.keys()))\n",
    "        Sim_Df = Sim_Df.fillna(0)\n",
    "        for column in list(self.params_dict.keys()):\n",
    "            if self.params_dict[column]['distribution'] == 'categorical':\n",
    "                data_sim = np.random.choice(a=self.params_dict[column]['parameters'][0],\n",
    "                                            p=self.params_dict[column]['parameters'][1],\n",
    "                                            size=size)\n",
    "                Sim_Df[column] = data_sim\n",
    "            else:\n",
    "                dist = getattr(scipy.stats, self.params_dict[column]['distribution'])\n",
    "                data_sim = dist.rvs(*self.params_dict[column]['parameters'], size=size)\n",
    "                Sim_Df[column] = data_sim\n",
    "        return Sim_Df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to calculate probabilities for each alternative **(to be replaced by functions from the choice_tools module in pylogit)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_intercept_to_df(df_long, specification_dict):\n",
    "    \n",
    "    if (\"intercept\" in specification_dict \n",
    "        and \"intercept\" not in df_long.columns):\n",
    "        df_long[\"intercept\"] = 1\n",
    "    \n",
    "    return None\n",
    "\n",
    "def create_design_matrix(df_long, specification_dict,\n",
    "                         names_dict, alternative_id_col):\n",
    "    \n",
    "    add_intercept_to_df(df_long,specification_dict)\n",
    "    \n",
    "    columns = []\n",
    "    for col in specification_dict:\n",
    "        for group in specification_dict[col]:\n",
    "            if type(group) == list:\n",
    "                columns.append(df_long[alternative_id_col].isin(group)\n",
    "                               *df_long[col])\n",
    "            else:\n",
    "                columns.append((df_long[alternative_id_col]==group)\n",
    "                               *df_long[col])\n",
    "    \n",
    "    design_matrix = np.stack(columns,axis = 1)\n",
    "    \n",
    "    var_names = []\n",
    "    for variable in names_dict:\n",
    "        for name in names_dict[variable]:\n",
    "            var_names.append(name)\n",
    "    \n",
    "    return design_matrix, var_names\n",
    "\n",
    "\n",
    "def calculate_utilities(betas, design_matrix):\n",
    "    \n",
    "    limit_max = 700\n",
    "    limit_min = -700 \n",
    "    \n",
    "    utility = design_matrix.dot(betas)\n",
    "    utility[utility>limit_max] = limit_max\n",
    "    utility[utility<limit_min] = limit_min\n",
    "    \n",
    "    utilities = np.exp(utility)\n",
    "    \n",
    "    return utilities\n",
    "\n",
    "\n",
    "def create_mapping_matrix(df_long, observation_id_col):\n",
    "    row_to_col_matrix = pd.get_dummies(df_long[observation_id_col]).values\n",
    "#     row_to_col_matrix = (df_long[observation_id_col].values[:,None] == \n",
    "#                          np.sort(df_long[observation_id_col].unique())[None,:]).astype(int) \n",
    "    sparse_row_to_col_matrix = sparse.csr_matrix(row_to_col_matrix)\n",
    "    \n",
    "    mapping_matrix = sparse_row_to_col_matrix.dot(sparse_row_to_col_matrix.T)\n",
    "    \n",
    "    return mapping_matrix\n",
    "\n",
    "\n",
    "def calculate_probabilities(betas,design_matrix, mapping_matrix):\n",
    "    \n",
    "    utilities = calculate_utilities(betas, design_matrix)\n",
    "    denominator = mapping_matrix.dot(utilities)\n",
    "    probabilities = utilities/denominator\n",
    "    probabilities[probabilities==0] = 1e-300\n",
    "    \n",
    "    \n",
    "    return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to simulate choices based on long data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SimulateChoices(data, alt_id_col, obs_id_col, number_alts, spec_dic, names_dic, init_betas):\n",
    "\n",
    "# This commented out block is to extend the capabilities of the function from simulating choices\n",
    "# only based on long data to wide data. The logic is built out completely, all we need is to \n",
    "# refactor it and adjust the parameters in the function definition.\n",
    "#     choice_var = 'choice'\n",
    "#     custom_alt_id = \"alternative_id\"\n",
    "#     obs_id_column = \"custom_id\"\n",
    "\n",
    "#     # Adding a Choice Variable Column\n",
    "#     data[choice_var] = np.random.randint(1, high=number_alts+1, size=data.shape[0])\n",
    "#     availability_variables = dict()\n",
    "\n",
    "#     # Adding availability variables and specifying alternative availability numbers\n",
    "#     for alt in np.arange(1, number_alts+1, 1):\n",
    "#         data['AV_' + str(alt)] = 1\n",
    "#         availability_variables[alt] = 'AV_' + str(alt)\n",
    "\n",
    "#     # Specifying the Individual Variables\n",
    "#     ind_variables = ind_variables\n",
    "\n",
    "#     alt_varying_variables = alt_var_vars\n",
    "#     # Specifying the column heading for the alternative id column in the\n",
    "#     # long format dataset\n",
    "#     custom_alt_id = \"alternative_id\"\n",
    "#     obs_id_column = \"custom_id\"\n",
    "#     sample_data[obs_id_column] = np.arange(sample_data.shape[0],\n",
    "#                                            dtype=int) + 1\n",
    "\n",
    "#     # Create a variable recording the choice column\n",
    "\n",
    "#     long_data = pl.convert_wide_to_long(wide_data=sample_data,\n",
    "#                                         ind_vars=ind_variables,\n",
    "#                                         alt_specific_vars=alt_varying_variables,\n",
    "#                                         availability_vars=availability_variables,\n",
    "#                                         obs_id_col=obs_id_column,\n",
    "#                                         choice_col=choice_var,\n",
    "#                                         new_alt_id_name=custom_alt_id)\n",
    "    sim_choice_var = 'sim_choice'\n",
    "    # Functions to generate the probabilities for each alternative\n",
    "\n",
    "    long_data = data\n",
    "\n",
    "    design_matrix, names = create_design_matrix(df_long=long_data,\n",
    "                                                specification_dict=mnl_specification,\n",
    "                                                names_dict=mnl_names,\n",
    "                                                alternative_id_col=alt_id_col)\n",
    "    mapping_matrix = create_mapping_matrix(df_long=long_data,\n",
    "                                           observation_id_col=obs_id_col)\n",
    "    probabilities = calculate_probabilities(betas=initial_betas,\n",
    "                                            design_matrix=design_matrix,\n",
    "                                            mapping_matrix=mapping_matrix)\n",
    "\n",
    "    data['probabilities'] = probabilities\n",
    "    data['cum_sum'] = 0\n",
    "    data['sim_choice'] = 0\n",
    "\n",
    "    for observation in data['observation_id'].unique():\n",
    "        probs_sum = data[data.observation_id==observation]['probabilities'].cumsum()\n",
    "        data.loc[data['observation_id']==observation,'cum_sum'] = probs_sum\n",
    "\n",
    "    observation_id_list = list(data.observation_id.unique())\n",
    "    u_random = np.random.uniform(size = len(data['observation_id'].unique()))\n",
    "\n",
    "    for u,obs in zip(u_random,observation_id_list):\n",
    "        data_sample = data[data['observation_id']==obs]\n",
    "        sorted_list = sorted(list(data_sample['mode_id'].unique()))\n",
    "        choices = dict.fromkeys(sorted_list, 0)\n",
    "        for alt in sorted_list:   \n",
    "            choices[alt] = np.where(u<=data_sample[data_sample['mode_id']==alt]['cum_sum'], 1, 0).item()\n",
    "            if choices[alt] == 1:\n",
    "                break\n",
    "        data.loc[data.observation_id==obs,'sim_choice'] = data['mode_id'].map(choices)\n",
    "    return data"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
