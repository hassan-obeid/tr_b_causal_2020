{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../src/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats\n",
    "from fitter import Fitter\n",
    "from collections import defaultdict, OrderedDict\n",
    "import pylogit as pl\n",
    "from functools import reduce\n",
    "import seaborn as sns\n",
    "import random\n",
    "from scipy import sparse\n",
    "import copy\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution Fitting Class Definition - **Could be removed** - For Documentation Purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class FitDistribution(object):\n",
    "#     \"\"\"Fit and simulate data to known distributions.\n",
    "\n",
    "#     Input:\n",
    "#     ------\n",
    "#     - data: array-like or dataframe.\n",
    "#     - dists: list.\n",
    "#         This parameter contains a list of distributions to be explored.\n",
    "#         When None, every available distribution on scipy is explored.\n",
    "#     - bins: int.\n",
    "#         Numbers of bins to be used for the cumulative histogram. This has\n",
    "#         an impact on the quality of the fit.\n",
    "#     - timeout: int.\n",
    "#         Maximum time for a given distribution. If timeout is reached,\n",
    "#         the distribution is skipped.\n",
    "#         \"\"\"\n",
    "#     def __init__(self, data, dists=None, timeout=30, verbose=False, bins=100):\n",
    "#         self.data = data\n",
    "#         # self.var_types = var_types\n",
    "#         self.dists = dists\n",
    "#         self.timeout = timeout\n",
    "#         self.verbose = verbose\n",
    "#         self.bins = bins\n",
    "#         self.ArrayDistDict = defaultdict()\n",
    "#         self.params_dict = defaultdict(dict)\n",
    "\n",
    "#     def FindArrayDist(self, cat_var):\n",
    "#         \"\"\"Function to extract the best distribution for a specified array.\n",
    "#         Uses the fit method from the Fitter module in the fitter library\n",
    "#         Inputs:\n",
    "#         -------\n",
    "#         - cat_var: boolean\n",
    "#             Boolean to signify whether the variable to be simulated\n",
    "#             is discrete/categorical or continuous.\n",
    "\n",
    "#         Outputs:\n",
    "#         -------\n",
    "#         By default, the function returns a dictionary with best distribution\n",
    "#         name and parameters associated with it. If a number of distributions\n",
    "#         was specified, the function returns a pandas DataFrame with\n",
    "#         the N best distributions, along with a plot showing all of them.\"\"\"\n",
    "#         self.ArrayDistDict = dict()\n",
    "#         if cat_var is True:\n",
    "#             self.ArrayDistDict['distribution'] = 'categorical'\n",
    "#             np_array_range = np.arange(self.data.max()+1)\n",
    "#             array_bincount = np.bincount(self.data)\n",
    "#             probs = array_bincount / len(self.data)\n",
    "\n",
    "#             self.ArrayDistDict['parameters'] = [np_array_range,\n",
    "#                                                 probs]\n",
    "#         else:\n",
    "#             fitter_object = Fitter(data=self.data,\n",
    "#                                    distributions=self.dists,\n",
    "#                                    timeout=self.timeout)\n",
    "#             fitter_object.fit()\n",
    "#             BestDict = fitter_object.get_best()\n",
    "#             self.ArrayDistDict['distribution'] = list(BestDict.items())[0][0]\n",
    "#             self.ArrayDistDict['parameters'] = list(BestDict.items())[0][1]\n",
    "#         return self.ArrayDistDict\n",
    "\n",
    "#     def SimArray(self, size=100):\n",
    "#         \"\"\"Function to simulate data for an array based on the best fitted\n",
    "#         distribution.\n",
    "#         Input:\n",
    "#         -----\n",
    "#         - size : int\n",
    "#                 size of the array to be simulated.\n",
    "#         Outputs:\n",
    "#         -------\n",
    "#         Simulated array based on the best fit distribution.\"\"\"\n",
    "#         if self.ArrayDistDict['distribution'] == 'categorical':\n",
    "#             value = self.ArrayDistDict['parameters'][0]\n",
    "#             freq = self.ArrayDistDict['parameters'][1]\n",
    "#             Sim_Array = np.random.choice(a=value,\n",
    "#                                          p=freq,\n",
    "#                                          size=size)\n",
    "#         else:\n",
    "#             dist = getattr(scipy.stats, self.ArrayDistDict['distribution'])\n",
    "#             Sim_Array = dist.rvs(*self.ArrayDistDict['parameters'], size=size)\n",
    "#         return Sim_Array\n",
    "\n",
    "#     def FindDfDist(self, var_types):\n",
    "#         \"\"\"Function to extract the best distribution from a specified\n",
    "#         dataframe. Uses the function find_dist, which in turn uses the\n",
    "#         fit method from the Fitter module in the fitter library\n",
    "#         Inputs:\n",
    "#         -------\n",
    "#         - var_types: dictionary\n",
    "#             Dictionary with keys as column names for dataset variables,\n",
    "#             the value of each key is a string showing whether the\n",
    "#             variable is discrete/cat or continuous.\n",
    "\n",
    "#         Outputs:\n",
    "#         -------\n",
    "#         *FOR NOW*, the function returns a dictionary showing the best\n",
    "#         distribution name for each array in the dataframe and parameters\n",
    "#         associated with it.\n",
    "#         \"\"\"\n",
    "\n",
    "#         for column in list(self.data.columns):\n",
    "\n",
    "#             if var_types[column] == 'categorical':\n",
    "#                 if len(self.data[column].unique()) == 1:\n",
    "#                     self.params_dict[column]['distribution'] = 'constant'\n",
    "#                     self.params_dict[column]['parameters'] = \\\n",
    "#                         self.data[column].unique()\n",
    "#                 else:\n",
    "#                     self.params_dict[column]['distribution'] = 'categorical'\n",
    "#                     np_array_range = np.arange(self.data[column].max()+1)\n",
    "#                     array_bincount = np.bincount(self.data[column])\n",
    "#                     probs = array_bincount / len(self.data[column])\n",
    "#                     self.params_dict[column]['parameters'] = [np_array_range,\n",
    "#                                                               probs]\n",
    "#             else:\n",
    "#                 if len(self.data[column].unique()) == 1:\n",
    "#                     self.params_dict[column]['distribution'] = 'constant'\n",
    "#                     self.params_dict[column]['parameters'] = \\\n",
    "#                         self.data[column].unique()\n",
    "#                 else:\n",
    "#                     fitter_object = Fitter(data=self.data[column],\n",
    "#                                            distributions=self.dists,\n",
    "#                                            timeout=self.timeout)\n",
    "#                     fitter_object.fit()\n",
    "#                     BestDict = fitter_object.get_best()\n",
    "#                     self.params_dict[column]['distribution'] = \\\n",
    "#                         list(BestDict.items())[0][0]\n",
    "#                     self.params_dict[column]['parameters'] = \\\n",
    "#                         list(BestDict.items())[0][1]\n",
    "#         return self.params_dict\n",
    "\n",
    "#     def SimDf(self, size=1000):\n",
    "#         \"\"\"Funtion to simulate data of size N based on specified\n",
    "#         distribution/parameters found by the fitter package.\n",
    "#         Inputs:\n",
    "#         -------\n",
    "#         data: dataframe from which columns are to be taken\n",
    "#         dist_params: the distribution parameters from find_dist_df\n",
    "#         Outputs:\n",
    "#         -------\n",
    "#         DataFrame object with simulated data based on specified distributions\n",
    "#         \"\"\"\n",
    "#         Sim_Df = pd.DataFrame(columns=list(self.params_dict.keys()))\n",
    "#         Sim_Df = Sim_Df.fillna(0)\n",
    "#         for column in list(self.params_dict.keys()):\n",
    "#             if self.params_dict[column]['distribution'] == 'categorical':\n",
    "#                 value = self.params_dict[column]['parameters'][0]\n",
    "#                 freq = self.params_dict[column]['parameters'][1]\n",
    "#                 data_sim = np.random.choice(a=value,\n",
    "#                                             p=freq,\n",
    "#                                             size=size)\n",
    "#                 Sim_Df[column] = data_sim\n",
    "#             elif self.params_dict[column]['distribution'] == 'constant':\n",
    "#                 data_sim = self.params_dict[column]['parameters'][0]\n",
    "#                 Sim_Df[column] = data_sim\n",
    "#             else:\n",
    "#                 dist = getattr(scipy.stats,\n",
    "#                                self.params_dict[column]['distribution'])\n",
    "#                 data_sim = dist.rvs(*self.params_dict[column]['parameters'],\n",
    "#                                     size=size)\n",
    "#                 Sim_Df[column] = data_sim\n",
    "#         return Sim_Df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to calculate probabilities for each alternative - **(Replaced by functions from TB)** - For Documentation Purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def add_intercept_to_df(df_long, specification_dict):\n",
    "#     \"\"\"Function to add intercept to long format DataFrame\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     df_long: DataFrame\n",
    "#         Long format Pandas DataFrame to which to add \n",
    "#         intercept column.\n",
    "#     specification_dict: dict\n",
    "#         Specification Dictionary for the model\n",
    "    \n",
    "#     Returns\n",
    "#     -------\n",
    "#     In-place Pandas DataFrame with additional intercept column.\n",
    "        \n",
    "#     \"\"\"\n",
    "#     if (\"intercept\" in specification_dict\n",
    "#             and \"intercept\" not in df_long.columns):\n",
    "#         df_long[\"intercept\"] = 1\n",
    "#     return None\n",
    "\n",
    "\n",
    "# def create_design_matrix(df_long, specification_dict,\n",
    "#                          names_dict, alternative_id_col):\n",
    "\n",
    "#     add_intercept_to_df(df_long, specification_dict)\n",
    "\n",
    "#     columns = []\n",
    "#     for col in specification_dict:\n",
    "#         for group in specification_dict[col]:\n",
    "#             if type(group) == list:\n",
    "#                 columns.append(df_long[alternative_id_col].isin(group)\n",
    "#                                * df_long[col])\n",
    "#             else:\n",
    "#                 columns.append((df_long[alternative_id_col] == group)\n",
    "#                                * df_long[col])\n",
    "\n",
    "#     design_matrix = np.stack(columns, axis=1)\n",
    "\n",
    "#     var_names = []\n",
    "#     for variable in names_dict:\n",
    "#         for name in names_dict[variable]:\n",
    "#             var_names.append(name)\n",
    "\n",
    "#     return design_matrix, var_names\n",
    "\n",
    "\n",
    "# def calculate_utilities(betas, design_matrix):\n",
    "\n",
    "#     limit_max = 700\n",
    "#     limit_min = -700\n",
    "\n",
    "#     utility = design_matrix.dot(betas)\n",
    "#     utility[utility > limit_max] = limit_max\n",
    "#     utility[utility < limit_min] = limit_min\n",
    "\n",
    "#     utilities = np.exp(utility)\n",
    "\n",
    "#     return utilities\n",
    "\n",
    "\n",
    "# def create_mapping_matrix(df_long, observation_id_col):\n",
    "#     row_to_col_matrix = pd.get_dummies(df_long[observation_id_col]).values\n",
    "# #     row_to_col_matrix = (df_long[observation_id_col].values[:,None] ==\n",
    "# #                          np.sort(df_long[observation_id_col].unique())[None,:]).astype(int)\n",
    "#     sparse_row_to_col_matrix = sparse.csr_matrix(row_to_col_matrix)\n",
    "\n",
    "#     mapping_matrix = sparse_row_to_col_matrix.dot(sparse_row_to_col_matrix.T)\n",
    "\n",
    "#     return mapping_matrix\n",
    "\n",
    "\n",
    "# def calculate_probabilities(betas, design_matrix, mapping_matrix):\n",
    "\n",
    "#     utilities = calculate_utilities(betas, design_matrix)\n",
    "#     denominator = mapping_matrix.dot(utilities)\n",
    "#     probabilities = utilities/denominator\n",
    "#     probabilities[probabilities == 0] = 1e-300\n",
    "\n",
    "#     return probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to simulate choices based on long data format - For Documentation Purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def SimulateChoices(long_data, alt_id_col,\n",
    "#                     obs_id_col, number_alts,\n",
    "#                     spec_dic, names_dic, init_betas):\n",
    "#     \"\"\"\n",
    "#     Function to simulate choices from a long data\n",
    "#     format dataset.\n",
    "    \n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     long_data : DataFrame\n",
    "#         The DataFrame to be used, in long format.\n",
    "#     alt_id_col: string\n",
    "#         Name of the column containing the alternative\n",
    "#         id numbers in the long format dataset.\n",
    "#     obs_id_col: string\n",
    "#         Name of the column containing the observation\n",
    "#         id numbers in the long format dataset.\n",
    "#     number_alts: int\n",
    "#         Number of alternatives in the long format\n",
    "#         dataset.\n",
    "#     spec_dic: dictionary\n",
    "#         Dictionary of the model specification.\n",
    "#     names_dic: dictionary\n",
    "#         Dictionary of the alternative names.\n",
    "#     init_betas: list\n",
    "#         List of the initial betas for the model\n",
    "#         from which the choices should be simulated.\n",
    "    \n",
    "#     Returns\n",
    "#     -------\n",
    "#     DataFrame object with the simulated choices column\n",
    "#     added as 'sim_choice'\n",
    "#     \"\"\"\n",
    "#     # Declare the simulated choice column name\n",
    "#     sim_choice_col = 'sim_choice'\n",
    "\n",
    "#     # Make a copy of the data\n",
    "#     data = copy.deepcopy(long_data)\n",
    "#     # Functions to generate the design matrix, mapping matrix,\n",
    "#     # and calculate the probabilities for each alternative\n",
    "#     design_matrix, names = create_design_matrix(df_long=data,\n",
    "#                                                 specification_dict=spec_dic,\n",
    "#                                                 names_dict=names_dic,\n",
    "#                                                 alternative_id_col=alt_id_col)\n",
    "#     mapping_matrix = create_mapping_matrix(df_long=data,\n",
    "#                                            observation_id_col=obs_id_col)\n",
    "#     probabilities = calculate_probabilities(betas=initial_betas,\n",
    "#                                             design_matrix=design_matrix,\n",
    "#                                             mapping_matrix=mapping_matrix)\n",
    "#     # Assign calculated probabilities to new dataframe column\n",
    "#     data['probabilities'] = probabilities\n",
    "#     # Initialize cumulative sum and simulated choice columns\n",
    "#     data['cum_sum'] = 0\n",
    "#     data['sim_choice'] = 0\n",
    "\n",
    "#     # Loop around the observations and compute probabilities' cumulative\n",
    "#     # sums for each alternative\n",
    "#     for observation in data['observation_id'].unique():\n",
    "#         probs_sum = data[long_data.observation_id == observation]['probabilities'].cumsum()\n",
    "#         data.loc[data['observation_id'] == observation, 'cum_sum'] = probs_sum\n",
    "\n",
    "#     # Generate list for observation ids to be used in simulating choices    \n",
    "#     observation_id_list = list(data.observation_id.unique())\n",
    "#     # Generate a \"random utility\" array of the same size as the number\n",
    "#     # of observations in the dataset\n",
    "#     u_random = np.random.uniform(size=len(data['observation_id'].unique()))\n",
    "\n",
    "#     # Loop around the generate utilities and observations in the dataset\n",
    "#     # to assign a choice to each\n",
    "#     for u, obs in zip(u_random, observation_id_list):\n",
    "#         # select data for observation number \"obs\"\n",
    "#         data_sample = data[data['observation_id'] == obs]\n",
    "#         # generate list of available modes for each observation\n",
    "#         sorted_list = sorted(list(data_sample['mode_id'].unique()))\n",
    "#         # initialize a dictionary from the available modes for \n",
    "#         # each observation\n",
    "#         choices = dict.fromkeys(sorted_list, 0)\n",
    "#         # Loop round the modes for each observation and assign \n",
    "#         # choice (0 vs. 1)\n",
    "#         for alt in sorted_list:\n",
    "#             choices[alt] = np.where(u <= data_sample[data_sample['mode_id']\n",
    "#                                                      == alt]\n",
    "#                                     ['cum_sum'], 1, 0).item()\n",
    "#             # Once a choice is assigned, break out of loop\n",
    "#             if choices[alt] == 1:\n",
    "#                 break\n",
    "#         # Map the choices for the observation to the long format dataframe       \n",
    "#         data.loc[data.observation_id == obs, sim_choice_col] = \\\n",
    "#             data['mode_id'].map(choices)\n",
    "#     return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Data - Based on Wide Data - For Documentation Purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert Data from Long to Wide before simulation\n",
    "\n",
    "# # Create variable list for subset of long data,\n",
    "# # I will add the remaining values from the long format dataset\n",
    "# alt_id_col = \"mode_id\"\n",
    "\n",
    "# obs_id_col = \"observation_id\"\n",
    "\n",
    "# choice_col = \"choice\"\n",
    "\n",
    "# # Store of columns relevant to the data to be simulated\n",
    "# model_variables = list(mnl_model.specification.keys())\n",
    "# model_variables.remove('intercept')\n",
    "# model_variables.extend([alt_id_col, obs_id_col,\n",
    "#                         choice_col, 'num_cars', 'num_licensed_drivers'])\n",
    "# print('The model variables of interest are:')\n",
    "# model_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a copy of the data subset\n",
    "# subset_bike_data = bike_data_long[model_variables].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create the needed variables for the conversion\n",
    "# ind_spec_vars = ['num_kids', 'household_size',\n",
    "#                  'cars_per_licensed_drivers', 'cross_bay',\n",
    "#                  'num_cars', 'num_licensed_drivers']\n",
    "\n",
    "# alt_spec_vars = ['total_travel_time', 'total_travel_distance']\n",
    "\n",
    "# subset_spec_vars = {'total_travel_cost': [1, 2, 3],\n",
    "#                     'cost_per_distance': [1, 2, 3]}\n",
    "\n",
    "# alternative_name_dict = {1: 'drive_alone',\n",
    "#                          2: 'shared_2',\n",
    "#                          3: 'shared_3p',\n",
    "#                          4: 'wtw',\n",
    "#                          5: 'dtw',\n",
    "#                          6: 'wtd',\n",
    "#                          7: 'walk',\n",
    "#                          8: 'bike'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert data from long to wide, I assigned a null value of 0\n",
    "# # because it will make it easier to simulate data when we have\n",
    "# # Unavailable variables\n",
    "# bike_data_wide = pl.convert_long_to_wide(long_data=subset_bike_data,\n",
    "#                                          ind_vars=ind_spec_vars,\n",
    "#                                          alt_specific_vars=alt_spec_vars,\n",
    "#                                          subset_specific_vars=subset_spec_vars,\n",
    "#                                          obs_id_col=obs_id_col,\n",
    "#                                          alt_id_col=alt_id_col,\n",
    "#                                          choice_col=choice_col,\n",
    "#                                          alt_name_dict=alternative_name_dict)\n",
    "# bike_data_wide.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we need to decide how we will simulate data when we have unavailable values. TBD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the list of variables of interest from data_wide\n",
    "# columns_wide = ['num_kids', 'household_size', 'num_cars',\n",
    "#                 'num_licensed_drivers', 'cross_bay',\n",
    "#                 'total_travel_time_drive_alone', 'total_travel_time_shared_2',\n",
    "#                 'total_travel_time_shared_3p', 'total_travel_time_wtw',\n",
    "#                 'total_travel_time_dtw', 'total_travel_time_wtd',\n",
    "#                 'total_travel_time_walk', 'total_travel_time_bike',\n",
    "#                 'total_travel_distance_drive_alone',\n",
    "#                 'total_travel_distance_shared_2',\n",
    "#                 'total_travel_distance_shared_3p',\n",
    "#                 'total_travel_distance_wtw',\n",
    "#                 'total_travel_distance_dtw', 'total_travel_distance_wtd',\n",
    "#                 'total_travel_distance_walk', 'total_travel_distance_bike',\n",
    "#                 'total_travel_cost_drive_alone', 'total_travel_cost_shared_2',\n",
    "#                 'total_travel_cost_shared_3p']\n",
    "\n",
    "# # Restrict data to desired columns\n",
    "# bike_data_wide = bike_data_wide[columns_wide]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Determine the distributions to be used\n",
    "# distributions = ['normal', 'alpha', 'beta', 'gamma', 'expon', 'gumbel']\n",
    "\n",
    "# # Initial the FitDistribution object\n",
    "# bike_data_fitter = FitDistribution(data=bike_data_wide, dists=distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the nature of each variables whether\n",
    "# # discrete/categorical or continuous\n",
    "# variable_type = {'num_kids': 'categorical',\n",
    "#                  'household_size': 'categorical',\n",
    "#                  'num_cars': 'discrete',\n",
    "#                  'num_licensed_drivers': 'categorical',\n",
    "#                  'cross_bay': 'categorical',\n",
    "#                  'total_travel_time_drive_alone': 'continuous',\n",
    "#                  'total_travel_time_shared_2': 'continuous',\n",
    "#                  'total_travel_time_shared_3p': 'continuous',\n",
    "#                  'total_travel_time_wtw': 'continuous',\n",
    "#                  'total_travel_time_dtw': 'continuous',\n",
    "#                  'total_travel_time_wtd': 'continuous',\n",
    "#                  'total_travel_time_walk': 'continuous',\n",
    "#                  'total_travel_time_bike': 'continuous',\n",
    "#                  'total_travel_distance_drive_alone': 'continuous',\n",
    "#                  'total_travel_distance_shared_2': 'continuous',\n",
    "#                  'total_travel_distance_shared_3p': 'continuous',\n",
    "#                  'total_travel_distance_wtw': 'continuous',\n",
    "#                  'total_travel_distance_dtw': 'continuous',\n",
    "#                  'total_travel_distance_wtd': 'continuous',\n",
    "#                  'total_travel_distance_walk': 'continuous',\n",
    "#                  'total_travel_distance_bike': 'continuous',\n",
    "#                  'total_travel_cost_drive_alone': 'continuous',\n",
    "#                  'total_travel_cost_shared_2': 'continuous',\n",
    "#                  'total_travel_cost_shared_3p': 'continuous'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Simulate dataframe based on the estimated distributions\n",
    "# sim_bike_data = bike_data_fitter.SimDf(size=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Distribution Based on Long Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def FindLongDataDist(data_long,\n",
    "                     alt_id_col,\n",
    "                     obs_id_col,\n",
    "                     alt_spec,\n",
    "                     alt_name_dic,\n",
    "                     ind_spec,\n",
    "                     trip_spec,\n",
    "                     var_types,\n",
    "                     cont_dists=None):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to find the distribution of specific variables\n",
    "    from a long format dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_long: Pandas DataFrame\n",
    "        Dataset in long format from which variable\n",
    "        distribution is to be found.\n",
    "        \n",
    "    alt_id_col: string\n",
    "        Name of the column with alternative ids.\n",
    "        \n",
    "    obs_id_col: string\n",
    "        Name of the column with observation ids.\n",
    "        \n",
    "    alt_spec: list\n",
    "        List containing strings of the names of\n",
    "        alternative specific variables.\n",
    "        \n",
    "    alt_name_dic: dictionary\n",
    "        Dictionary with keys as the ordered number\n",
    "        of alternatives, and the value for each key\n",
    "        is a string representing the name of the \n",
    "        alternative.\n",
    "        \n",
    "    ind_spec: list\n",
    "        List containing strings of the names of \n",
    "        individual specific variables.\n",
    "        \n",
    "    trip_spec: list\n",
    "        List containing string of the names of \n",
    "        trip specific variables.\n",
    "        \n",
    "    var_types: dictionary\n",
    "        Dictionary with keys as strings of names of variables\n",
    "        from long format dataset, and values for each key are\n",
    "        the type of variables (e.g.: 'categorical vs. continuous').\n",
    "        \n",
    "    cont_dists: list\n",
    "        List of continuous RVs distribution names from scipy.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    a nested dictionary with keys as variable names and values\n",
    "    as dictionaries containing both the distribution name and\n",
    "    its parameters.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the output parameters dictionary\n",
    "    params_dict = defaultdict(dict)\n",
    "\n",
    "    # Loop around individual specific variables\n",
    "    for ind in ind_spec:\n",
    "        # generate array of values for individual specific variable\n",
    "        ind_var = pd.Series([(data_long.loc[data_long[obs_id_col] == x][ind].unique()[0]) for x in data_long[obs_id_col].unique()])\n",
    "        # Get distribution if variable is categorical\n",
    "        if var_types[ind] == 'categorical':\n",
    "            # If only one category\n",
    "            if len(ind_var.unique()) == 1:\n",
    "                params_dict[ind]['distribution'] = 'constant'\n",
    "                params_dict[ind]['parameters'] = ind_var.unique()\n",
    "            # If more than one category\n",
    "            else:\n",
    "                params_dict[ind]['distribution'] = 'categorical'\n",
    "                # Count frequency of values and store it as paramater of distribution\n",
    "                np_array_range = np.arange(ind_var.max()+1)\n",
    "                array_bincount = np.bincount(ind_var)\n",
    "                probs = array_bincount / len(ind_var)\n",
    "                params_dict[ind]['parameters'] = [np_array_range,\n",
    "                                                  probs]\n",
    "        else:\n",
    "            # If not categorical but just one unique value\n",
    "            if len(ind_var.unique()) == 1:\n",
    "                params_dict[ind]['distribution'] = 'constant'\n",
    "                params_dict[ind]['parameters'] = ind_var.unique()\n",
    "            # If not categorical but not one unique value\n",
    "            else:\n",
    "                # Use the Fitter library to fit distributions\n",
    "                # to the data\n",
    "                fitter_object = Fitter(data=ind_var,\n",
    "                                       distributions=cont_dists,\n",
    "                                       timeout=30)\n",
    "                fitter_object.fit()\n",
    "                # Get the best distribution and store in dictionary\n",
    "                BestDict = fitter_object.get_best()\n",
    "                params_dict[ind]['distribution'] = list(BestDict.items())[0][0]\n",
    "                params_dict[ind]['parameters'] = list(BestDict.items())[0][1]\n",
    "\n",
    "    # Code for Alternative Specific Variables\n",
    "    # Loop around the different available alternatives\n",
    "    for alt in data_long[alt_id_col].unique():\n",
    "        # Store data for specific alternative (mode)\n",
    "        mode_data = data_long.loc[data_long['mode_id'] == alt]\n",
    "        # Loop around the alternative specific variables in the input dictionary\n",
    "        for var in alt_spec:\n",
    "            # If data is categorical\n",
    "            if var_types[var] == 'categorical':\n",
    "                # If only one category\n",
    "                if len(mode_data[var].unique()) == 1:\n",
    "                    # Add name of alternative to variable and store distriburion & parameters\n",
    "                    params_dict[var+'_'+alt_name_dic[alt]]['distribution'] = 'constant'\n",
    "                    params_dict[var+'_'+alt_name_dic[alt]]['parameters'] = mode_data[var].unique()\n",
    "                else:\n",
    "                    # If more than one category, compute the frequency of values\n",
    "                    # and store as parameters\n",
    "                    # Add name of alternative to variable and store distriburion & parameters\n",
    "                    params_dict[var+'_'+alt_name_dic[alt]]['distribution'] = 'categorical'\n",
    "                    np_array_range = np.arange(mode_data[var].max()+1)\n",
    "                    array_bincount = np.bincount(mode_data[var])\n",
    "                    probs = array_bincount / len(mode_data[var])\n",
    "                    params_dict[var+'_'+alt_name_dic[alt]]['parameters'] = [np_array_range,\n",
    "                                                                            probs]\n",
    "            else:\n",
    "                # If data is not categorical but has one unique value\n",
    "                if len(mode_data[var].unique()) == 1:\n",
    "                    # Add name of alternative to variable and store distriburion & parameters\n",
    "                    params_dict[var+'_'+alt_name_dic[alt]]['distribution'] = 'constant'\n",
    "                    params_dict[var+'_'+alt_name_dic[alt]]['parameters'] = mode_data[var].unique()\n",
    "                # If data is not categorical but has more than one unique value\n",
    "                else:\n",
    "                    # Use the Fitter library to fit distributions\n",
    "                    # to the data\n",
    "                    fitter_object = Fitter(data=mode_data[var],\n",
    "                                           distributions=cont_dists,\n",
    "                                           timeout=30)\n",
    "                    fitter_object.fit()\n",
    "                    # Get the best distribution and store in dictionary\n",
    "                    BestDict = fitter_object.get_best()\n",
    "                    # Add name of alternative to variable and store distriburion & parameters\n",
    "                    params_dict[var+'_'+alt_name_dic[alt]]['distribution'] = list(BestDict.items())[0][0]\n",
    "                    params_dict[var+'_'+alt_name_dic[alt]]['parameters'] = list(BestDict.items())[0][1]\n",
    "\n",
    "    # Trip Specific Variable (maybe combine with individual specific variables)\n",
    "    # Loop around trip (observation) specific variables\n",
    "    for var in trip_spec:\n",
    "        # generate array of values for trip specific variable\n",
    "        trip_var = pd.Series([(data_long.loc[data_long[obs_id_col] == x][var].unique()[0]) for x in data_long[obs_id_col].unique()])\n",
    "        # Get distribution if variable is categorical\n",
    "        if var_types[var] == 'categorical':\n",
    "            if len(trip_var.unique()) == 1:\n",
    "            # If only one category\n",
    "                params_dict[var]['distribution'] = 'constant'\n",
    "                params_dict[var]['parameters'] = trip_var.unique()\n",
    "            else:\n",
    "            # If more than one category\n",
    "                params_dict[var]['distribution'] = 'categorical'\n",
    "            # Count frequency of values and store it as paramater of distribution\n",
    "                np_array_range = np.arange(trip_var.max()+1)\n",
    "                array_bincount = np.bincount(trip_var)\n",
    "                probs = array_bincount / len(trip_var)\n",
    "                params_dict[var]['parameters'] = [np_array_range,\n",
    "                                                  probs]\n",
    "        else:\n",
    "            # If not categorical but just one unique value\n",
    "            if len(trip_var.unique()) == 1:\n",
    "                params_dict[var]['distribution'] = 'constant'\n",
    "                params_dict[var]['parameters'] = trip_var.unique()\n",
    "            # If not categorical but just one unique value\n",
    "            else:\n",
    "                # Use the Fitter library to fit distributions\n",
    "                # to the data\n",
    "                fitter_object = Fitter(data=trip_var,\n",
    "                                       distributions=cont_dists,\n",
    "                                       timeout=30)\n",
    "                fitter_object.fit()\n",
    "                # Get the best distribution and store in dictionary\n",
    "                BestDict = fitter_object.get_best()\n",
    "                params_dict[var]['distribution'] = list(BestDict.items())[0][0]\n",
    "                params_dict[var]['parameters'] = list(BestDict.items())[0][1]\n",
    "\n",
    "    return params_dict\n",
    "\n",
    "\n",
    "def SimDf(params_dict, size=1000):\n",
    "    \"\"\"\n",
    "    Funtion to simulate data of size N based on specified\n",
    "    distribution/parameters found by the fitter package.\n",
    "    \n",
    "    Paremeters\n",
    "    ----------\n",
    "    dist_params: dictionary\n",
    "        The variable distribution dictionary resulting from\n",
    "        `FindLongDataDist`.\n",
    "        \n",
    "    size: int\n",
    "        Size of the desired simulated dataset, default value\n",
    "        is 1000 observations.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DataFrame object with simulated data based on specified distributions\n",
    "    \n",
    "    \"\"\"\n",
    "    # Create Empty DataFrame with keys from params_dict\n",
    "    Sim_Df = pd.DataFrame(columns=list(params_dict.keys()))\n",
    "    Sim_Df = Sim_Df.fillna(0)\n",
    "    \n",
    "    # Loop around each of the variables in params_dict\n",
    "    # and simulate data for them\n",
    "    for column in list(params_dict.keys()):\n",
    "        if params_dict[column]['distribution'] == 'categorical':\n",
    "            data_sim = np.random.choice(a=params_dict[column]['parameters'][0],\n",
    "                                        p=params_dict[column]['parameters'][1],\n",
    "                                        size=size)\n",
    "            Sim_Df[column] = data_sim\n",
    "        elif params_dict[column]['distribution'] == 'constant':\n",
    "            data_sim = params_dict[column]['parameters'][0]\n",
    "            Sim_Df[column] = data_sim\n",
    "        else:\n",
    "            dist = getattr(scipy.stats, params_dict[column]['distribution'])\n",
    "            data_sim = dist.rvs(*params_dict[column]['parameters'], size=size)\n",
    "            Sim_Df[column] = data_sim\n",
    "    return Sim_Df\n",
    "\n",
    "\n",
    "def SimulateAvailability(data_long, sim_data, obs_id_col, alt_name_dict):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function to simulate alternative availability based on a long format\n",
    "    dataset and join the availability data to the simulated dataset\n",
    "    resulting from SimDf.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_long: Pandas DataFrame\n",
    "        Long format dataframe used for simulating\n",
    "        alternative availability.\n",
    "    \n",
    "    sim_data: Pandas DataFrame\n",
    "        Wide format dataframe resulting from SimDf\n",
    "    \n",
    "    obs_id_col: string\n",
    "        Name of the column in data_long with \n",
    "        observation ids.\n",
    "    \n",
    "    alt_name_dic: dictionary\n",
    "        Dictionary with keys as the ordered number\n",
    "        of alternatives, and the value for each key\n",
    "        is a string representing the name of the \n",
    "        alternative.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    Wide format Pandas DataFrame with additional availability\n",
    "    columns for each of the alternatives.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    # Create empty Series to hold number of available alternatives\n",
    "    series = pd.Series([])\n",
    "    \n",
    "    # Loop around each observation to record the\n",
    "    # available number of alternatives for each observation\n",
    "    for i, obs in zip(np.arange(len(data_long[obs_id_col].unique())), data_long[obs_id_col].unique()):\n",
    "        series[i] = data_long[data_long[obs_id_col] == obs].shape[0]\n",
    "\n",
    "    # Simulate number of available alternatives for\n",
    "    # each observation in sim_data\n",
    "    av_size = sim_data.shape[0]\n",
    "    alts_sim = np.random.choice(a=np.arange(series.max()+1),\n",
    "                                p=np.bincount(series)/len(series),\n",
    "                                size=av_size)\n",
    "\n",
    "    # simulate the availability matrix based on number \n",
    "    # of available alternatives\n",
    "    N = len(alt_name_dict)\n",
    "    av_sim = [np.array([1] * K + [0]*(N-K)) for K in alts_sim]\n",
    "\n",
    "    # Shuffle the available alternatives for each observation\n",
    "    # because av_sim will always start with 1s\n",
    "    for x in av_sim:\n",
    "        np.random.shuffle(x)\n",
    "\n",
    "    # Shuffle the availability across different observations\n",
    "    np.random.shuffle(av_sim)\n",
    "    \n",
    "    # Create columns for the availability matrix\n",
    "    AV_columns = [alt_name_dict[i]+'_AV' for i in alt_name_dict.keys()]\n",
    "    \n",
    "    # Create alternative availability matrix with AV_columns\n",
    "    AV_Df = pd.DataFrame(av_sim, columns=AV_columns)\n",
    "    \n",
    "    # Create an random choice column based on available\n",
    "    # alternatives for each observation - This column will\n",
    "    # be needed when converting to long data\n",
    "    fake_choice = [random.choice(np.nonzero(a == 1)[0]) + 1 for a in np.array(AV_Df)]\n",
    "    fake_choice_df = pd.DataFrame(fake_choice, columns=['sim_choice'])\n",
    "    \n",
    "    # Concatenate the simulated data with availability data and fake choice data\n",
    "    # and return Sim_DF_AV\n",
    "    Sim_DF_AV = pd.concat([sim_data, AV_Df, fake_choice_df], axis=1, sort=False)\n",
    "    return Sim_DF_AV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example using bike data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingestion and Exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable for the path to the long format data for\n",
    "# the multinomial choice model\n",
    "PATH = '../../data/raw/spring_2016_all_bay_area_long_format_plus_cross_bay_col.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The columns of bike_data are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['household_id', 'person_id', 'tour_id', 'observation_id', 'mode_id',\n",
       "       'choice', 'tour_origin_taz', 'primary_dest_taz', 'total_travel_time',\n",
       "       'total_travel_cost', 'total_travel_distance', 'age', 'household_size',\n",
       "       'household_income', 'household_income_values', 'transit_subsidy',\n",
       "       'transit_subsidy_amount', 'num_cars', 'num_licensed_drivers',\n",
       "       'cross_bay', 'oakland_and_berkeley', 'survey_id', 'gender',\n",
       "       'non_relative_flag', 'num_pre_school', 'num_school_aged', 'married',\n",
       "       'parent', 'income_category_1', 'income_category_2', 'income_category_3',\n",
       "       'income_category_4', 'income_category_5', 'income_category_6',\n",
       "       'income_category_7', 'income_category_8', 'income_category_9',\n",
       "       'income_category_10', 'income_unknown', 'ln_drive_cost',\n",
       "       'ln_drive_cost_sq', 'total_travel_time_10x', 'total_travel_time_tenth',\n",
       "       'high_income', 'medium_income', 'low_income', 'high_income_cost',\n",
       "       'medium_income_cost', 'low_income_cost', 'unknown_income_cost',\n",
       "       'high_income_ln_cost', 'medium_income_ln_cost', 'low_income_ln_cost',\n",
       "       'unknown_income_ln_cost', 'cars_per_licensed_drivers', 'num_kids',\n",
       "       'family_in_household', 'married_woman', 'cost_per_distance',\n",
       "       'intercept', 'cost_per_distance_10$_per_mi',\n",
       "       'total_travel_time_tenth_min'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading data from the specified PATH\n",
    "bike_data_long = pd.read_csv(PATH)\n",
    "\n",
    "# If in previous work we accidentally saved the index with the dataframe\n",
    "# remove the old index from the data\n",
    "if \"Unnamed: 0\" in bike_data_long.columns:\n",
    "    del bike_data_long[\"Unnamed: 0\"]\n",
    "\n",
    "print(\"The columns of bike_data are:\")\n",
    "bike_data_long.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the mode shares in the data set\n",
    "alt_id_to_mode_name = {1: \"Drive Alone\",\n",
    "                       2: \"Shared Ride 2\",\n",
    "                       3: \"Shared Ride 3+\",\n",
    "                       4: \"Walk-Transit-Walk\",\n",
    "                       5: \"Drive-Transit-Walk\",\n",
    "                       6: \"Walk-Transit-Drive\",\n",
    "                       7: \"Walk\",\n",
    "                       8: \"Bike\"}\n",
    "\n",
    "mode_counts = bike_data_long.loc[bike_data_long.choice == 1,\n",
    "                                 \"mode_id\"].value_counts().loc[range(1, 9)]\n",
    "\n",
    "mode_shares = mode_counts / bike_data_long.observation_id.max()\n",
    "mode_shares.index = [alt_id_to_mode_name[x] for x in mode_shares.index.values]\n",
    "mode_shares.name = \"Mode Shares\"\n",
    "mode_shares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNL Model Specification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create my specification and variable names for the basic MNL model\n",
    "# NOTE: - Keys should be variables within the long format dataframe.\n",
    "#         The sole exception to this is the \"intercept\" key.\n",
    "#       - For the specification dictionary, the values should be lists\n",
    "#         or lists of lists. Within a list, or within the inner-most\n",
    "#         list should be the alternative ID's of the alternative whose\n",
    "#         utility specification the explanatory variable is entering.\n",
    "\n",
    "mnl_specification = OrderedDict()\n",
    "mnl_names = OrderedDict()\n",
    "\n",
    "mnl_specification[\"intercept\"] = [2, 3, 4, 5, 6, 7, 8]\n",
    "mnl_names[\"intercept\"] = ['ASC Shared Ride: 2',\n",
    "                          'ASC Shared Ride: 3+',\n",
    "                          'ASC Walk-Transit-Walk',\n",
    "                          'ASC Drive-Transit-Walk',\n",
    "                          'ASC Walk-Transit-Drive',\n",
    "                          'ASC Walk',\n",
    "                          'ASC Bike']\n",
    "\n",
    "mnl_specification[\"total_travel_time\"] = [[1, 2, 3], [4, 5, 6]]\n",
    "mnl_names[\"total_travel_time\"] = ['Travel Time, units:min (All Auto Modes)',\n",
    "                                  'Travel Time, units:min (All Transit Modes)']\n",
    "\n",
    "mnl_specification[\"total_travel_cost\"] = [[4, 5, 6]]\n",
    "mnl_names[\"total_travel_cost\"] = ['Travel Cost, units:$ (All Transit Modes)']\n",
    "\n",
    "mnl_specification[\"cost_per_distance\"] = [1, 2, 3]\n",
    "mnl_names[\"cost_per_distance\"] = [\"Travel Cost per Distance, units:$/mi (Drive Alone)\",\n",
    "                                  \"Travel Cost per Distance, units:$/mi (SharedRide-2)\",\n",
    "                                  \"Travel Cost per Distance, units:$/mi (SharedRide-3+)\"]\n",
    "\n",
    "mnl_specification[\"cars_per_licensed_drivers\"] = [[1, 2, 3]]\n",
    "mnl_names[\"cars_per_licensed_drivers\"] = [\"Autos per licensed drivers (All Auto Modes)\"]\n",
    "\n",
    "mnl_specification[\"total_travel_distance\"] = [7, 8]\n",
    "mnl_names[\"total_travel_distance\"] = ['Travel Distance, units:mi (Walk)',\n",
    "                                      'Travel Distance, units:mi (Bike)']\n",
    "\n",
    "# mnl_specification[\"cross_bay\"] = [[2, 3], [4, 5, 6]]\n",
    "# mnl_names[\"cross_bay\"] = [\"Cross-Bay Tour (Shared Ride 2 & 3+)\",\n",
    "#                           \"Cross-Bay Tour (All Transit Modes)\"]\n",
    "mnl_specification[\"cross_bay\"] = [[2, 3]]\n",
    "mnl_names[\"cross_bay\"] = [\"Cross-Bay Tour (Shared Ride 2 & 3+)\"]\n",
    "\n",
    "mnl_specification[\"household_size\"] = [[2, 3]]\n",
    "mnl_names[\"household_size\"] = ['Household Size (Shared Ride 2 & 3+)']\n",
    "\n",
    "mnl_specification[\"num_kids\"] = [[2, 3]]\n",
    "mnl_names[\"num_kids\"] = [\"Number of Kids in Household (Shared Ride 2 & 3+)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the basic MNL model, using the hessian and newton-conjugate gradient\n",
    "mnl_model = pl.create_choice_model(data=bike_data_long,\n",
    "                                   alt_id_col=\"mode_id\",\n",
    "                                   obs_id_col=\"observation_id\",\n",
    "                                   choice_col=\"choice\",\n",
    "                                   specification=mnl_specification,\n",
    "                                   model_type=\"MNL\",\n",
    "                                   names=mnl_names)\n",
    "\n",
    "num_vars = len(reduce(lambda x, y: x + y, mnl_names.values()))\n",
    "\n",
    "# Note newton-cg used to ensure convergence to a point where gradient\n",
    "# is essentially zero for all dimensions.\n",
    "mnl_model.fit_mle(np.zeros(num_vars),\n",
    "                  method=\"BFGS\")\n",
    "\n",
    "# Look at the estimation results\n",
    "mnl_model.get_statsmodels_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaring all the needed variables for each of our functions\n",
    "\n",
    "# Observation id column\n",
    "observation_id_col = 'observation_id'\n",
    "\n",
    "# Alternative id column\n",
    "alternative_id_col = 'mode_id'\n",
    "\n",
    "# Individual specific variables list\n",
    "individual_specific_variables = ['num_kids', 'household_size',\n",
    "                                 'num_cars', 'num_licensed_drivers']\n",
    "\n",
    "# Alternative specific variables list\n",
    "alternative_specific_variables = ['total_travel_time',\n",
    "                                  'total_travel_distance',\n",
    "                                  'total_travel_cost']\n",
    "\n",
    "# Trip specific variables list\n",
    "trip_specific_variables = ['cross_bay']\n",
    "\n",
    "# Alternative name dictionary\n",
    "alternative_name_dict = {1: 'drive_alone',\n",
    "                         2: 'shared_2',\n",
    "                         3: 'shared_3p',\n",
    "                         4: 'wtw',\n",
    "                         5: 'dtw',\n",
    "                         6: 'wtd',\n",
    "                         7: 'walk',\n",
    "                         8: 'bike'}\n",
    "\n",
    "# Variable type Dictionary\n",
    "variable_type = {'num_kids': 'categorical',\n",
    "                 'household_size': 'categorical',\n",
    "                 'num_cars': 'categorical',\n",
    "                 'num_licensed_drivers': 'categorical',\n",
    "                 'cross_bay': 'categorical',\n",
    "                 'total_travel_time': 'continuous',\n",
    "                 'total_travel_distance': 'continuous',\n",
    "                 'total_travel_cost': 'continuous'}\n",
    "\n",
    "# Distribution to be explored for continuous variables\n",
    "distributions = ['normal', 'alpha', 'beta', 'gamma', 'expon', 'gumbel']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a parameters dictionary from long\n",
    "# format data\n",
    "bike_data_params = FindLongDataDist(data_long=bike_data_long,\n",
    "                                    alt_id_col=alternative_id_col,\n",
    "                                    obs_id_col=observation_id_col,\n",
    "                                    alt_spec=alternative_specific_variables,\n",
    "                                    alt_name_dic=alternative_name_dict,\n",
    "                                    ind_spec=individual_specific_variables,\n",
    "                                    trip_spec=trip_specific_variables,\n",
    "                                    var_types=variable_type,\n",
    "                                    cont_dists=distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate data from the params_dict\n",
    "wide_sim_data = SimDf(params_dict=bike_data_params, size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the simulated data\n",
    "wide_sim_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate availability, add fake choice column\n",
    "# and return final simulated data with availability\n",
    "# and choices\n",
    "\n",
    "wide_sim_data_availability = SimulateAvailability(bike_data_long,\n",
    "                                     sim_data=wide_sim_data,\n",
    "                                     obs_id_col=observation_id_col,\n",
    "                                     alt_name_dict=alternative_name_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate Choices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Simulated Data from Wide to Long "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_variables = ['num_kids', 'household_size',\n",
    "                 'num_cars', 'num_licensed_drivers', 'cross_bay']\n",
    "\n",
    "\n",
    "# Dictionary of Alternative Specific Variables\n",
    "# TODO: verify whether all variables are needed\n",
    "# for each alternative\n",
    "alt_varying_variables = {u'total_travel_time': dict([(1, 'total_travel_time_drive_alone'),\n",
    "                                                     (2, 'total_travel_time_shared_2'),\n",
    "                                                     (3, 'total_travel_time_shared_3p'),\n",
    "                                                     (4, 'total_travel_time_wtw'),\n",
    "                                                     (5, 'total_travel_time_dtw'),\n",
    "                                                     (6, 'total_travel_time_wtd'),\n",
    "                                                     (7, 'total_travel_time_walk'),\n",
    "                                                     (8, 'total_travel_time_bike')]),\n",
    "                         u'total_travel_cost': dict([(1, 'total_travel_cost_drive_alone'),\n",
    "                                                     (2, 'total_travel_cost_shared_2'),\n",
    "                                                     (3, 'total_travel_cost_shared_3p'),\n",
    "                                                     (4, 'total_travel_cost_wtw'),\n",
    "                                                     (5, 'total_travel_cost_dtw'),\n",
    "                                                     (6, 'total_travel_cost_wtd'),\n",
    "                                                     (7, 'total_travel_cost_walk'),\n",
    "                                                     (8, 'total_travel_cost_bike')]),\n",
    "                         u'total_travel_distance': dict([(1, 'total_travel_distance_drive_alone'),\n",
    "                                                         (2, 'total_travel_distance_shared_2'),\n",
    "                                                         (3, 'total_travel_distance_shared_3p'),\n",
    "                                                         (4, 'total_travel_distance_wtw'),\n",
    "                                                         (5, 'total_travel_distance_dtw'),\n",
    "                                                         (6, 'total_travel_distance_wtd'),\n",
    "                                                         (7, 'total_travel_distance_walk'),\n",
    "                                                         (8, 'total_travel_distance_bike')]),\n",
    "                            }\n",
    "\n",
    "\n",
    "# Dictionary of alternative availability variables\n",
    "availability_variables = {1: 'drive_alone_AV',\n",
    "                          2: 'shared_2_AV',\n",
    "                          3: 'shared_3p_AV',\n",
    "                          4: 'wtw_AV',\n",
    "                          5: 'dtw_AV',\n",
    "                          6: 'wtd_AV',\n",
    "                          7: 'walk_AV',\n",
    "                          8: 'bike_AV'}\n",
    "\n",
    "##########\n",
    "# Determine the columns for: alternative ids, the observation ids and the choice\n",
    "##########\n",
    "# The 'custom_alt_id' is the name of a column to be created in the long-format data\n",
    "# It will identify the alternative associated with each row.\n",
    "custom_alt_id = \"mode_id\"\n",
    "\n",
    "# Create a custom id column that ignores the fact that this is a\n",
    "# panel/repeated-observations dataset. Note the +1 ensures the id's start at one.\n",
    "obs_id_column = \"observation_id\"\n",
    "\n",
    "wide_sim_data_availability[obs_id_column] = np.arange(wide_sim_data.shape[0],\n",
    "                                         dtype=int) + 1\n",
    "\n",
    "\n",
    "# Create an empty choice column\n",
    "choice_column = \"sim_choice\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Long Format Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data from wide to long\n",
    "long_sim_data = pl.convert_wide_to_long(wide_sim_data_availability,\n",
    "                                        ind_variables,\n",
    "                                        alt_varying_variables,\n",
    "                                        availability_variables,\n",
    "                                        obs_id_column,\n",
    "                                        choice_column,\n",
    "                                        new_alt_id_name=custom_alt_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a cars per licensed drivers column\n",
    "long_sim_data[\"cars_per_licensed_drivers\"] = 0\n",
    "long_sim_data.loc[long_sim_data.num_licensed_drivers > 0,\n",
    "                  \"cars_per_licensed_drivers\"] = long_sim_data.num_cars / long_sim_data.num_licensed_drivers.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a variable representing cost divided by distance\n",
    "long_sim_data[\"cost_per_distance\"] = 0\n",
    "long_sim_data.loc[long_sim_data.mode_id.isin([1, 2, 3]),\n",
    "                  \"cost_per_distance\"] = (long_sim_data.loc[long_sim_data.mode_id.isin([1, 2, 3]),\n",
    "                                                            \"total_travel_cost\"] /\n",
    "                                          long_sim_data.loc[long_sim_data.mode_id.isin([1, 2, 3]),\n",
    "                                                            \"total_travel_distance\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulate choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probabilities for each alternative\n",
    "# based on the estimated model\n",
    "posterior_probs = mnl_model.predict(long_sim_data)\n",
    "\n",
    "# Simulate choice data\n",
    "long_sim_data['sim_choice'] = viz.simulate_choice_vector(posterior_probs,\n",
    "                               long_sim_data['observation_id'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate the model based on simulate choice data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate the basic MNL model, using the hessian and newton-conjugate gradient\n",
    "mnl_model_sim = pl.create_choice_model(data=long_sim_data_choice,\n",
    "                                       alt_id_col=\"mode_id\",\n",
    "                                       obs_id_col=\"observation_id\",\n",
    "                                       choice_col=\"sim_choice\",\n",
    "                                       specification=mnl_specification,\n",
    "                                       model_type=\"MNL\",\n",
    "                                       names=mnl_names)\n",
    "\n",
    "num_vars = len(reduce(lambda x, y: x + y, mnl_names.values()))\n",
    "# Note newton-cg used to ensure convergence to a point where gradient\n",
    "# is essentially zero for all dimensions.\n",
    "mnl_model_sim.fit_mle(np.zeros(num_vars),\n",
    "                      method=\"BFGS\")\n",
    "\n",
    "# Look at the estimation results\n",
    "mnl_model_sim.get_statsmodels_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Repeat simulation many times "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up all needed variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_id_col = 'observation_id'\n",
    "\n",
    "alternative_id_col = 'mode_id'\n",
    "\n",
    "variable_type = {'num_kids': 'categorical',\n",
    "                 'household_size': 'categorical',\n",
    "                 'num_cars': 'categorical',\n",
    "                 'num_licensed_drivers': 'categorical'}\n",
    "\n",
    "individual_specific_variables = ['num_kids', 'household_size',\n",
    "                                 'num_cars', 'num_licensed_drivers']\n",
    "\n",
    "alternative_specific_variables = ['total_travel_time', 'total_travel_distance', 'total_travel_cost']\n",
    "\n",
    "trip_specific_variables = ['cross_bay']\n",
    "\n",
    "alternative_name_dict = {1: 'drive_alone',\n",
    "                         2: 'shared_2',\n",
    "                         3: 'shared_3p',\n",
    "                         4: 'wtw',\n",
    "                         5: 'dtw',\n",
    "                         6: 'wtd',\n",
    "                         7: 'walk',\n",
    "                         8: 'bike'}\n",
    "\n",
    "variable_type = {'num_kids': 'categorical',\n",
    "                 'household_size': 'categorical',\n",
    "                 'num_cars': 'categorical',\n",
    "                 'num_licensed_drivers': 'categorical',\n",
    "                 'cross_bay': 'categorical',\n",
    "                 'total_travel_time': 'continuous',\n",
    "                 'total_travel_distance': 'continuous',\n",
    "                 'total_travel_cost': 'continuous'}\n",
    "\n",
    "distributions = ['normal', 'alpha', 'beta', 'gamma', 'expon', 'gumbel']\n",
    "\n",
    "choice_column = \"sim_choice\"\n",
    "\n",
    "custom_alt_id = \"mode_id\"\n",
    "\n",
    "alt_varying_variables = {u'total_travel_time': dict([(1, 'total_travel_time_drive_alone'),\n",
    "                                                     (2, 'total_travel_time_shared_2'),\n",
    "                                                     (3, 'total_travel_time_shared_3p'),\n",
    "                                                     (4, 'total_travel_time_wtw'),\n",
    "                                                     (5, 'total_travel_time_dtw'),\n",
    "                                                     (6, 'total_travel_time_wtd'),\n",
    "                                                     (7, 'total_travel_time_walk'),\n",
    "                                                     (8, 'total_travel_time_bike')]),\n",
    "                         u'total_travel_cost': dict([(1, 'total_travel_cost_drive_alone'),\n",
    "                                                     (2, 'total_travel_cost_shared_2'),\n",
    "                                                     (3, 'total_travel_cost_shared_3p'),\n",
    "                                                     (4, 'total_travel_cost_wtw'),\n",
    "                                                     (5, 'total_travel_cost_dtw'),\n",
    "                                                     (6, 'total_travel_cost_wtd'),\n",
    "                                                     (7, 'total_travel_cost_walk'),\n",
    "                                                     (8, 'total_travel_cost_bike')]),\n",
    "                         u'total_travel_distance': dict([(1, 'total_travel_distance_drive_alone'),\n",
    "                                                         (2, 'total_travel_distance_shared_2'),\n",
    "                                                         (3, 'total_travel_distance_shared_3p'),\n",
    "                                                         (4, 'total_travel_distance_wtw'),\n",
    "                                                         (5, 'total_travel_distance_dtw'),\n",
    "                                                         (6, 'total_travel_distance_wtd'),\n",
    "                                                         (7, 'total_travel_distance_walk'),\n",
    "                                                         (8, 'total_travel_distance_bike')]),\n",
    "                        }\n",
    "\n",
    "\n",
    "availability_variables = {1: 'drive_alone_AV',\n",
    "                          2: 'shared_2_AV',\n",
    "                          3: 'shared_3p_AV',\n",
    "                          4: 'wtw_AV',\n",
    "                          5: 'dtw_AV',\n",
    "                          6: 'wtd_AV',\n",
    "                          7: 'walk_AV',\n",
    "                          8: 'bike_AV'}\n",
    "\n",
    "bike_data_params = FindLongDataDist(data_long=bike_data_long,\n",
    "                                    alt_id_col=alternative_id_col,\n",
    "                                    obs_id_col=observation_id_col,\n",
    "                                    alt_spec=alternative_specific_variables,\n",
    "                                    alt_name_dic=alternative_name_dict,\n",
    "                                    ind_spec=individual_specific_variables,\n",
    "                                    trip_spec=trip_specific_variables,\n",
    "                                    var_types=variable_type,\n",
    "                                    cont_dists=distributions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_size = np.random.randint(low=2000, high=3000, size=100)\n",
    "sim_number = np.arange(1,101)\n",
    "models_dictionary = defaultdict(dict)\n",
    "\n",
    "for size, number in zip(simulation_size, sim_number):\n",
    "    print('Simulation number', number , 'is in process...')\n",
    "    print('------------------------------------------')\n",
    "\n",
    "    # Simulate data from the params_dict\n",
    "    wide_sim_data = SimDf(params_dict=bike_data_params,\n",
    "                     size = size)\n",
    "\n",
    "    # Simulate availability based on availability \n",
    "    # from long data and addd a fake choice column\n",
    "    wide_sim_data_availability = SimulateAvailability(data_long=bike_data_long, \n",
    "                                         sim_data=sim_data, \n",
    "                                         obs_id_col=observation_id_col, \n",
    "                                         alt_name_dict=alternative_name_dict)\n",
    "\n",
    "    # Add an observation_id column\n",
    "    wide_sim_data[obs_id_column] = np.arange(wide_sim_data.shape[0],\n",
    "                                            dtype=int) + 1\n",
    "    \n",
    "    # Convert simulated data from wide to long\n",
    "    long_sim_data = pl.convert_wide_to_long(wide_data=wide_sim_data,\n",
    "                                            ind_vars=ind_variables, \n",
    "                                            alt_specific_vars=alt_varying_variables, \n",
    "                                            availability_vars=availability_variables,\n",
    "                                            obs_id_col=observation_id_col,\n",
    "                                            choice_col=choice_column,\n",
    "                                            new_alt_id_name=custom_alt_id)\n",
    "    \n",
    "    # Create a cars per licensed drivers column\n",
    "    long_sim_data[\"cars_per_licensed_drivers\"] = 0\n",
    "    long_sim_data.loc[long_sim_data.num_licensed_drivers > 0,\n",
    "                      \"cars_per_licensed_drivers\"] = long_sim_data.num_cars / long_sim_data.num_licensed_drivers.astype(float)\n",
    "\n",
    "    # Add a variable representing cost divided by distance\n",
    "    long_sim_data[\"cost_per_distance\"] = 0\n",
    "    long_sim_data.loc[long_sim_data.mode_id.isin([1, 2, 3]),\n",
    "                      \"cost_per_distance\"] = (long_sim_data.loc[long_sim_data.mode_id.isin([1, 2, 3]),\n",
    "                                                                \"total_travel_cost\"] /\n",
    "                                              long_sim_data.loc[long_sim_data.mode_id.isin([1, 2, 3]),\n",
    "                                                        \"total_travel_distance\"])\n",
    "    \n",
    "    # Calculate posterior probabilities for each alternative\n",
    "    posterior_probs = mnl_model.predict(long_sim_data)\n",
    "\n",
    "    # Simulate choices based on calculated probabilities\n",
    "    long_sim_data['sim_choice'] = simulate_choice_vector(posterior_probs,\n",
    "                                                         long_sim_data['observation_id'].values)\n",
    "\n",
    "    # Estimate the basic MNL model, using the hessian and newton-conjugate gradient\n",
    "    mnl_model_sim = pl.create_choice_model(data=long_sim_data,\n",
    "                                           alt_id_col=alternative_id_col,\n",
    "                                           obs_id_col=observation_id_col,\n",
    "                                           choice_col=choice_column,\n",
    "                                           specification=mnl_specification,\n",
    "                                           model_type=\"MNL\",\n",
    "                                           names=mnl_names)\n",
    "\n",
    "    num_vars = len(reduce(lambda x, y: x + y, mnl_names.values()))\n",
    "    \n",
    "    # Note newton-cg used to ensure convergence to a point where gradient \n",
    "    # is essentially zero for all dimensions. \n",
    "    mnl_model_sim.fit_mle(np.zeros(num_vars),\n",
    "                          method=\"BFGS\")\n",
    "\n",
    "    # Store model\n",
    "    models_dictionary[number] = mnl_model_sim\n",
    "    \n",
    "    print('Simulation number', number , 'is complete!')\n",
    "    print('==========================================')\n",
    "    print('==========================================')"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
