\section{(Initial) Causal Graph Construction}
\label{sec:graph-construction}

Construction of an initial causal graph typically proceeds as follows.
At a high level, we
\begin{enumerate}
   \item adopt a population perspective,
   \item brainstorm all variables that we think affect the system that generates our observations,
   \item remove any variables that could cause bias in our causal inferences,
   \item connect all variables in our graph according to our a-priori beliefs about causal relations amongst them
   \item consider how the graph structure may differ between individuals and subgroups within the population.
\end{enumerate}
The following paragraphs describe these steps in detail.

To begin, we adopt the position of a researcher concerned about population level relationships.
This means we will think through what is a likely generative model for all individuals.
Later in this section, we will devote time to thinking about how subgroups and individual heterogeneity may affect our causal graphs.

Now, we add our first variable(s) to our graph, the outcome variable(s) of interest in our problem.
Note that we should consider relations and dependencies between these outcomes, and we should draw these onto our graphs.
Such inter-outcome dependencies may be of great relevance or even focus.
Recall, for example, the case of activity-based modellers that was mentioned in Section \ref{sec:choice-graphs}.
Similarly, medical researchers with data on multiple health measures or companies with multiple business metrics may all be interested in how the outcomes cause each other.

After adding our outcomes to our graph, we list all the variables we believe to cause them.
We refer to these influencing variables as our initial explanatory variables.

Next, we iterate through these initial explanatory variables.
For each current explanatory variable in the iteration, we think of variables that may modify the effect of the current explanatory variable on the outcome(s) of interest.
We refer to these variables as effect modifiers\footnote{Note, effect modifiers and confounders are easily confused. Both variables cause the outcome. The difference is that effect modifiers do not cause the explanatory / treatment variables. Confounders do. For discussion and classification of the different types of effect modification, based on one's causal graph, see \citet{vanderweele_2007_four}}.
Note that some effect modifiers may be a part of our list of initial explanatory variables.
For any effect modifiers that we think of, outside of the list of initial explanatory variables, we add them to our causal graph.

Overall, modifiers are important because our treatment effects systematically vary with them.
Accordingly, if we better understand when our treatments will be effective, then we can better target them.
% For instance, the effect of a transit voucher on increasing an individual's probability of using transit is likely modified by the recipients income.
% We expect a smaller treatment effect on wealthy individual's than on individual's with low income
For instance, imagine that a region-wide lockdown reduces the 14-day rolling average of new COVID-19 cases by X\% (on average).
Of course, we know that a lockdown's effectiveness is modified by the percentage of workers who must continue going out to work.
If most residents in an area are essential workers, then a lockdown will be less effective there, as compared with other locales.
We might wish to target other interventions for that region, as a replacement or supplement for the lockdown.
Targeting aside, knowledge of modifiers is also crucial to generalizing treatment effect inferences from one population to another.
To credibly transport our inferences, we must know what variables cause the treatment effects to differ between populations, and we must know how the distributions of those variables differs across populations \citep{pearl_2014_external}.
In general, see \citet{zheng_2018_automated} for a thorough introduction to moderation, its differences from other variables in one's causal graph, and for instructions on how to find moderators through data analysis alone.

After adding explanatory and effect modifying variables to the graph, we turn our attention to mediating variables.
A mediating variable is one through which an explanatory variable influences our outcome(s) of interest.
Such variables have multiple uses.
Under certain instances of confounding, mediators enable the ``front-door'' criterion to identify one's causal effect \citep{glynn_2018_front, bellemare_2019_paper, gupta_2020_estimating}.
Similarly, subject to particular causal assumptions, mediating variables permit inference on long-term outcomes of a selected intervention, given only its short-term proxies \citep{athey_2019_estimating, yang_2020_targeting}.

To find these mediators, we again iterate through each explanatory variable.
On each iteration, we brainstorm variables along paths of influence from our explanatory variable to our outcome.
For instance, consider how the presence of a bike lane influences bicycle mode choice.
We hypothesize that an individual's subjective perception of safety is the primary (or sole) mediator through which bicycle lane presence influences mode choice.
Accordingly, we add subjective perception of safety to our causal graph for travel mode choice.

After considering the variables above, we turn our attention to variables that complicate our analyses.
To begin with, we think of confounding variables.
The process is similar to how we generated effect modifying variables.
We iterate through each of the explanatory, mediating, and effect modifying variables, thinking specifically of any variables that both cause the current variable in the iteration and cause the outcome variable(s).
We call these variables, which cause our outcome and current variables in the iteration, confounding variables \citep{elwert_2013_graphical, greenland_1999_confounding}.
As an example, consider a person's attitude towards environmental conservation.
This attitude may cause both that individual's observed distance to their workplace (another explanatory variable) and that individual's choice of travel mode.
Both in this example and in general, we should add such confounding variables to our causal graph.

Next, we consider the effects of selection.
As noted by \citet{greenland_2020_causal}, all datasets have a causal graph that implicitly conditions on a selection node.
I.e., we only analyze data that has been selected to be a part of our dataset.
We should therefore consider how all of the other nodes in our causal graph relate to the selection node.
In particular, will we suffer any selection bias due to the outcomes influencing whether an observation is selected for inclusion in our dataset?
Selection bias, if present, can cause our estimated causal effects to differ greatly from their population counterparts.
This stems from systematic differences between the observations that have been selected into our dataset and the observations in our population of interest.
For more details, see \citet{heckman_1979_sample} and \citet{hernan_2004_structural} as canonical references.

Another universally implied yet only implicitly described element of one's causal graph is the prior data and code that led to one's dataset \citep[Pg.7]{greenland_2020_causal}.
Presumably, prior data and potentially code-enabled-analysis influenced the sample design that led to your dataset.
Perhaps some data transformations and code to implement those transformations was used to convert a raw dataset into the dataset being used for causal inference.
And at all times, one uses computer programs to compute your reported results.
In each case, the prior data is variable that influences your current data, and your code is computational (sub)graph that is implicit in your causal graph.
These elements should perhaps be made explicit, and their influence on your causal effect estimates should definitely be assessed and reported.

Next, we should explicitly consider the role of time, even in research that may be cross-sectional due to the data that is available to us or due to the problem itself.
In reality, how do we think our system evolves over time?
If we consider multiple observations of a given decision maker, how does that decision maker's observed variables at time $t$ partially cause future variables important to the context or outcome(s) for that decision maker at time $t' > t$?
How do the actions of a decision maker $i$ at time $t$ partially cause the future context or outcomes of a decision maker $j$?
We should add explicit nodes to our graph, subscripted or denoted by time, to show the cross-time causal relationships in our system.
For in-depth discussion of time-related causal inference topics, see papers such as \citet{gill_2001_causal}, \citet{eichler_2007_granger}, and \citet{peters_2013_causal}.
Please note that the literature on this topic is vast, and the cited authors are not at all exhaustive or representative of all papers in this space.
Interested readers are encouraged to perform further literature searches on their own.

Similarly, we will frequently want to consider the role of space.
In the context of choice modelling, this includes questions such as ``how does a decision maker's existence in a particular geographical area shape their choices?''
For example, consider multinational corporations where the business operates differently across state borders.
Here, the borders associated with space directly causes a difference in the causal graph of how these businesses deliver their goods and services to customers.
In other instances, one can consider space as a noisy proxy for unobserved confounders, such as cultural attitudes of a region's inhabitants \citep{paciorek_2010_importance}.
Rooted in the causal problem of unobserved confounding, this results in known statistical issues for choice modellers such as spatial correlation of model residuals \citep{fleming_2004_techniques}.
Fortunately, some progress has been made in dealing with such problems.
For general discussion of recent techniques in causal inference for spatial-causal modelling to deal with such issues, see \citet{osama_2019_inferring}.

At this point, we have added to our causal graph all the
outcome, explanatory, effect modifying, mediating, confounding, selection, data/code, time-indexed, and space-indexed variables that we believe are relevant for our problem.
However, many of these variables may be disconnected nodes, i.e., singletons in the graph.
We now focus on pruning nodes from this graph, before drawing our final hypothesized connections.
In particular, we focus on pruning ``post-outcome'' variables that are not part of the causal graph for future time periods or other observations.
The reason for this is that conditioning on such post-outcome variables would bias our causal effect estimates.
Our pruning exception is the selection variable which we have no choice but to condition on.
It may unfortunately be a post-outcome.

To remove the problematic variables, we iterate through each of the non-outcome variables in our graph, and we assess whether each variable is actually a result of the outcome (perhaps in combination with other variables in our graph).
These post-outcome variables temporally follow the outcome variable(s) but do not cause variables in the causal graph for other observations.
We remove all such post-outcome variables from our graph.

Now is a good time to step back and consider what other researchers have thought about our problem.
Specifically, we should conduct a literature review to see how other researchers have conceptualized the topic that we are working on.
Have they included variables that we have not?
Were those variables related our outcomes of interest?
If so, should we add these variables to our causal graph? How should these variables enter our graph?
Do the included variables of other researchers suggest the existence of confounders in their work that we should include in our graph?
Have other researchers ascribed differing roles to our graph's current variables than we have?
For example, have other researchers judged a variable to be a confounder, when we solely thought of the variable as an effect modifier?
As we answer these questions, we should critically examine the evidence for these alternative decisions to see if we should also reconsider how we're judging our variables.

Finally, we need to connect the variables in our graph.
\begin{enumerate}
   \item Draw direct arrows from our explanatory variables, confounders, and effect modifiers to the outcomes.
   \item Draw arrows from the explanatory variables to the mediators, and then draw arrows from the mediators to the outcomes.
   \item Draw arrows from the confounders to the explanatory variables and mediators that they may cause.
   \item Draw arrows from the variables in time $t$ to the variables that they cause in time $t+1$.
   \item Draw arrows from the variables that cause one's location in space to spatial variable nodes, and draw arrows to variables that are caused by one's location in space.
   \item Draw arrows from all other nodes in one's graph to the selection node, based on which variables cause inclusion in one's dataset.
\end{enumerate}
After drawing in all arrows, we should now have a fully connected causal graph.
Pause.
Take a moment to look over the graph to ensure there are no remaining singletons and that we have not drawn any spurious connections.
Then, take a moment to celebrate.
Drawing a project's first causal graph is hard work!

After celebrating, take a moment to pursue the following graph editing exercises.
First, think about how the graph might differ across sub-populations.
What sub-populations, if any, exist in your population of interest?
Are there any causal relationships that should, or should not, not exist for a given sub-population?
For instance, are the outcomes in some sub-populations independent of a given explanatory variable?
Can you think of any inverted causal relationships that are specific to this sub-population?
(I.e., for a given sub-population, does $B \rightarrow A$ instead of $A \rightarrow B$?)
As explained in \citet[Sec. 4]{druzdzel_2003_combining},
add these sub-populations to one's initial causal graph via a ``selection node'',
or if this is not clear enough,
draw modified causal graphs for each sub-population of interest.
Now, one can actually relax.
This concludes the ``purely mental'' drafting of one's causal graph.
In the next section, we'll look at testing this graph against data, and making any edits deemed empirically necessary.
