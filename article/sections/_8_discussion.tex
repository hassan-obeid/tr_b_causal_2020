\section{Discussion}
\label{sec:discussion}

In this chapter, we've focused on why causal graphs are important, how to create them, how to test them, and on how to use them in applied problems with latent confounding.
Our latent confounding example showed that statistical inference of our models' parameters may still be a challenge, even with a correct causal graph.
Such challenges lead us to the following set of post-graph-construction topics:
\begin{itemize}
   \item model estimation
   \item model checking
   \item experimental design
   \item experiment analysis
   \item decision analysis
\end{itemize}
To us, each item above is important for getting credible results from our analysis and for maximizing our interventions' benefits.
Accordingly, even though we will neglect details due to space and time constraints, we briefly discuss these topics below.

Perhaps most obviously, after creating and testing a causal graph, we will use it to estimate the effects of our interventions.
To compute our effect estimates, we will need to evaluate statements such as the probability of a particular node's variable taking a given value, conditional on the values of that node's parents.
These probabilities will come from our estimated models, so model estimation is critically important to our effect estimation.
Thankfully, this is the part of causal inference that choice \textit{modellers} are most familiar with.
For instance, \citet{kostic_2020_uncovering} first use the PC and GES causal discovery algorithms to generate a causal graph, then they test the graph qualitatively, and finally they estimate models corresponding to this causal graph.
As another example, \citet{garrido_2020_estimating} start from where we end: at a known (or selected) causal graph.
They then use neural network density estimators to model the necessary probabilities for estimating one's causal effects.

Next, after estimating the models for our causal graph but before interpreting or using our results, we should check our entire system of models.
Here, there are multiple, non-competing ways of performing these diagnostics.
We can check our models separately, jointly, or in subsets.
And if we're being thorough, we can perform all these checks instead of one kind.

From a disaggregate perspective, we can consider a sequential application of model checking exercises, one per estimated model.
Hopefully, each model checking process will include the use of visual diagnostics, as (for example) described in \citet{brathwaite_2018_check}.
Alternatively, we can follow a subset approach used by \citet{tran_2016_model}.
There, we jointly check the models for all variables that our intervention will set (i.e. the treatment assignment variables), and then we separately check the model(s) for our outcome variable(s).
Generally, we can check subsets of models together instead of checking one model at a time.
Finally, as in \citet{williamson_2013_model}, we may define global diagnostic measures over all nodes in our causal graph.
This corresponds to checking all our models jointly, and it represents yet another diagnostic approach.

Following model diagnostics, we are ready to use our models and causal graph to inform real interventions.
These interventions can come in two kind: an experiment or a ``full-scale'' implementation of one's policy.
If we are setting up an experiment, then we are typically interested in one of two aims.
We either want to decide between one or more treatment options, or we want to learn about our system, though not necessarily to make a decision.
In both cases, however, we pay great attention to the design of our experiments.

When experimenting to make decisions, such as whether to launch a given treatment or not, we pay extra attention to the size of our experiment.
Specifically, we want our experiment's sample size to be large enough such that after we update our beliefs using the experimental data, that we have at least our minimum desired probability of making the correct decision.
The decision can be to declare the effect of a treatment statistically different from zero, but more frequently, the decision will be more fundamental such as "implement treatment A."
For thorough explanations of how to conceptualize and design experiments in a bayesian, model-based setting, see \citet{chaloner_1995_bayesian} and \citet{wang_2002_simulation}.
For examples and guidance on how to use one's causal graph structure to guide the general design of one's experiment, beyond sample size, see \citet{madrigal_2007_cluster}.
There, the structure of one's causal graph is used to inform general design decisions such as the clustered allocation of individuals to treatment, and the experimental design is itself analyzed graphically.
Additionally, note the relations to reinforcement learning where an agent has to perform experiments in order to discover the action that will maximize her reward.
There, \citet{lee_2018_structural} have shown that designing our experiments without guidance from one's causal model is generally suboptimal, and that we can achieve optimality by leveraging our causal graph to design our experimentation plan.

Now, let's transition from decision making to consider experimentation for learning.
Imagine that we are at a transportation network company and that we are running a pricing experiment to learn about price elasticities of their customers.
Here, there is no immediate decision being made, but we learn about an edge in our causal graph: the edge from price of a trip (treatment) to purchase of the ride (outcome).
In other cases, we may experiment to learn not just about the strength of an edge, but about the presence of edges and the structure of the graph more generally.
For example, we may wish to remove residual ambiguity from a causal discovery process that output a Markov Equivalence Class of graphs instead of a single causal DAG.
In these situations, we're interested in optimally designing an experiment (or series of experiments) to learn a causal graph (or its properties), and we're interested in how we can leverage potentially mutliple experimental datasets to improve our causal graphs.
For a review of the literature on experimentation for learning and construction/refinement of a preliminary causal graph, see \citet[Sec. 3.1.2]{kalisch_2014_causal}.
For more recent approaches in this vein, see works such as \citet{triantafillou_2015_constraint, kocaoglu_2017_experimental, brouillard_2020_differentiable, rantanen_2020_learning}.
