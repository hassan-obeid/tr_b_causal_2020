\section{(Observable) Testing of Causal Graphs}

\subsection{Description}
In the last section, we reviewed a process for creating an initial causal graph using expert opinion.
Critically, after drafting a causal graph, we should immediately test it against empirical data.
Indeed, if our graph captures inaccurate assumptions about the data generating process, then we have no reason to think that our conclusions from using the graph will be accurate.

To test our causal graphs against data, we will first test the implications of our graph that involve observable variables only.
We will defer the task of testing implications that involve unobserved / latent variables to Section \ref{sec:latent-confounding}.
For now, recall our discussion in Section \ref{sec:graph-overview} about the two basic implications of causal graphs: marginal independence and conditional independence.
In both cases, direct testing of marginal or conditional independence amongst nodes in the causal graph may be difficult.
Indeed, there are no direct tests of conditional independence that can detect all types of dependence, especially for continuous variables \citep{bergsma_2004_testing, shah_2020_hardness}.

As a result of this hardness, there are a myriad of research efforts aimed at testing conditional independence of two variables $X$ and $Y$, given a third variable $Z$ and any number of assumptions about the variables or the test statistic itself.
Some researchers proceed under the assumption that one has access to an approximation of the conditional distribution of $X \mid Z$ \citep{candes_2018_panning, berrett_2019_conditional}.
Other researchers designed conditional independence tests for general cases, assuming smoothness of the underlying data distributions and assuming accurate estimation of the distribution of the test statistic under the null hypothesis of conditional independence (e.g. \citet{zhang_2012_kernel, strobl_2019_approximate}).

Different from (but not excluding) these approaches, we will take an easier and less decisive route.
If a pair of variables have conditionally or marginally independent distributions, then their statistical moments will also be conditionally or marginally independent.
Instead of testing for marginal or conditional independence in distribution, we will perform a more tractable test for marginal or conditional independence in means.
If the variables in question are not conditionally or marginally independent in their means, then we know they are not independent in their distributions.
Conversely, even if a set of variables are marginally or conditionally independent in their means, this \textbf{does not} imply that the variables are independent in distribution.

This approach of indirectly assessing distributional independence by testing mean independence is not new.
The following papers have all proposed and implemented such an idea: \citet{burkart_2017_predictive, chalupka_2018_fast, inacio_2019_conditional}.
For conditional independences, the crux of the approach is to predict $Y$ based on $X$ and $Z$, then compare against a prediction of $Y$ based on a resampled value of $X$ and the original $Z$.
If $Y$ is mean-independent of $X$ given $Z$, i.e. $E \left[ Y \mid X, Z \right] = E\left[ Y \mid Z \right]$, then the predictive power of a model with resampled $X$ should resemble the predictive power of a model with the original $X$.
After all, in both cases, the conditional expectation of $Y$ is independent of one's $X$ values (real or resampled).
When assessing marginal independencies, one removes $Z$ from the models for the expectation of $Y$ and proceeds as described.

Note that as with the case of testing distributional independence, testing mean independence still requires researchers to make choices.
We have to select models for $E \left[ Y \mid X, Z \right]$ and $E\left[ Y \mid Z \right]$, respectively, for testing conditional and marginal mean-independence.
We also have to choose the performance statistic (e.g. $R^2$, log-likelihood, etc.) to compare these models.
Lastly, we also have to select a resampling method.
In particular, how (if at all) will our resampling strategy account for the possible dependence between $X$ and $Z$?

% State the choices made in this work.

% Provide some discussion of the alternatives and the effects of different choices.
For our demonstration, we made the following choices.
First, we used linear regressions to model $E \left[Y \mid X, Z \right]$ and $E \left[ Y \mid Z \right]$.
Secondly, we chose to use $R^2$ as our test statistic for judging the predictive performance.
Finally, we have chosen to resample the $X$ vectors without replacement, keeping the length of the resampled vector equal to the length of the original vector.
In other words, we permute $X$.

Our rationale for these choices are as follows.
In our dataset, most of our explanatory variables were continuous (at least in theory).
Accordingly, $R^2$ seemed a sensible performance metric for a model of the conditional expectation of a continuous random variable.

In contrast to our choice of performance metric, we chose our conditional expectation models and resampling methods based on empirical testing.
In particular, we created simulations to assess our mean-independence testing procedure.
We assessed the performance of our mean-independence testing procedures using simulations where $Y \leftarrow Z \rightarrow X$ and $X$ either did or did not cause $Y$.

Of particular importance were our simulations under the null hypothesis where $X$ was conditionally independent of $Y$.
Our initial simulations used random forests as our conditional expectation models and permutations as resampling methods.
Random forests are a non-parametric method that would allow us to have less fear of model misspecification, and permutations are easy to implement.
However, under the null hypothesis, we discovered that the tests based on the random forest models did not result in p-values that were uniformly distributed.
Given that we planned to use these procedures in a manner akin to hypothesis-testing, we hoped that our test statistics would be U-statistics.
When we switched from the combination of random forests and permutations to linear regressions and permutations, our p-values were indeed empirically, uniformly distributed.
Moreover, we still retained high power.

We do not claim that these choices for assessing mean independence will always be appropriate.
Indeed, one should assess one's tests on simulated data that resembles one's real data.
For our illustrative purposes though, the combination of linear regressions, permutations, and $R^2$ resulted in adequate tests of marginal and conditional mean-independence.

\subsection{Demonstration}
% Illustrative causal graph
To demonstrate the testing procedures described above, we used the causal graph in Figure \ref{fig:graph-for-testing}.
This causal graph shows a set of hypothesized causal relationships between variables thought to contribute to the utility of the drive-alone travel alternative in our dataset.
As drawn, this graph encodes multiple marginal and conditional independence assumptions.
Tools such as Daggity \citep{textor_2016_robust} can be used to automatically infer all independencies based on one's graph.
For didactic purposes, however, we focused our attention on two particular independence assumptions.

First, we tested the assumption of marginal independence between the number of licensed drivers and the number of automobiles in a household.
A-priori, we may expect this independence to be have low-probability since the number of automobiles may generally be positively related to the number of licensed drivers in a household.
Secondly, we tested the assumption of that travel cost was independent of travel time, conditional on travel distance.
Unlike the first independence that we focus on, this conditional independence assumption is a-priori more credible.
Despite their differing levels of a-priori credibility, we will see how we can test both using the procedures described in the previous subsection.

% Marginal testing results
In particular, Figure \ref{fig:marginal-independence-test} shows the results of using permutation, linear regression, and $R^2$ to test the hypothesis of marginal independence between the number of automobiles and the number of licensed drivers in a household.
The empirical p-value of 0 confirms that the observed data is unlikely given the null-hypothesis of marginal independence.
More specifically, when regressing the number of licensed drivers in a household on the number of cars in that household, one achieves an $R^2$ near 0.4.
In contrast, when permuting the number of cars in the household and re-estimating the regression, the distribution of p-values concentrates around 0.
This plot visualizes the fact that, through the lens of our chosen test statistic ($R^2$), data generated under an assumption of marginal independence does not ``look like'' the observed data.
Accordingly, we should consider the weaker assumption of marginal dependence since this may lead to data-generating assumptions that offer greater realism and concordance with our observations.

% Conditional testing results
